
# Complete Fixed Raga Classification System
# Handles concatenated raga names and all previous issues

# Part 1: Setup and Imports (Updated with SpeechPy)

# Fix matplotlib backend FIRST before any other imports
import matplotlib
matplotlib.use('Agg')  # Use Anti-Grain Geometry backend (no GUI, no threading issues)

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Disable interactive plotting to prevent threading issues
plt.ioff()

# Import Parselmouth for MFCCs, Spectral Centroid, Spectral Bandwidth
try:
    import parselmouth
    PARSELMOUTH_AVAILABLE = True
    print("‚úÖ Parselmouth available")
except ImportError:
    PARSELMOUTH_AVAILABLE = False
    print("‚ùå Parselmouth not available. Install with: pip install praat-parselmouth")

# Import SpeechPy for Chroma, Tempo, ZCR
try:
    import speechpy
    SPEECHPY_AVAILABLE = True
    print("‚úÖ SpeechPy available")
except ImportError:
    SPEECHPY_AVAILABLE = False
    print("‚ùå SpeechPy not available. Install with: pip install speechpy")

# Try librosa as fallback for audio loading and some features
try:
    import librosa
    LIBROSA_AVAILABLE = True
    print("‚úÖ librosa available")
except ImportError:
    LIBROSA_AVAILABLE = False
    print("‚ö†Ô∏è librosa not available")

# Try scipy for signal processing
try:
    from scipy.signal import find_peaks
    import scipy.io.wavfile as wavfile
    SCIPY_AVAILABLE = True
    print("‚úÖ scipy available")
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è scipy not available")

# Set plotting style after backend is configured
plt.style.use('default')
sns.set_style("whitegrid")
sns.set_palette("Set1")

# Create directories
os.makedirs('results/json_values', exist_ok=True)
os.makedirs('results/graphs', exist_ok=True)
os.makedirs('results/graphs/features', exist_ok=True)
os.makedirs('results/graphs/analysis', exist_ok=True)
os.makedirs('results/graphs/visualizations', exist_ok=True)

print("‚úÖ Enhanced setup complete with SpeechPy!")

# Part 2: Fixed Data Loading and Preparation

def load_datasets():
    """Load both datasets with proper path handling"""
    print("üìÅ Loading datasets...")
    
    # Use the exact paths provided
    dataset1_path = "data/Dataset.csv"
    dataset2_path = "data/Final_dataset_s.csv"
    
    # Check if files exist
    if not os.path.exists(dataset1_path):
        print(f"‚ùå {dataset1_path} not found")
        return None, None
    
    if not os.path.exists(dataset2_path):
        print(f"‚ùå {dataset2_path} not found") 
        return None, None
    
    try:
        df1 = pd.read_csv(dataset1_path)
        df2 = pd.read_csv(dataset2_path)
        
        print(f"‚úÖ Dataset.csv: {df1.shape}")
        print(f"   Columns: {list(df1.columns)}")
        print(f"‚úÖ Final_dataset_s.csv: {df2.shape}")
        print(f"   Columns: {list(df2.columns)}")
        
        return df1, df2
        
    except Exception as e:
        print(f"‚ùå Error loading datasets: {e}")
        return None, None

def extract_individual_ragas(concatenated_string):
    """Extract individual raga names from concatenated string"""
    
    # Known raga names from the dataset
    known_ragas = [
        'Bhairav', 'Marwa', 'Malkauns', 'Hindol', 'Bilawal', 'Todi', 
        'Chandrakauns', 'Madhuvanti', 'Shree', 'Khamaj', 'Yaman', 
        'Kafi', 'Bhairavi', 'Kedar'
    ]
    
    # Convert to string and clean
    text = str(concatenated_string).strip()
    
    # If it's already a single raga name, return it
    if text in known_ragas:
        return text
    
    # Count occurrences of each raga in the concatenated string
    raga_counts = {}
    for raga in known_ragas:
        count = text.count(raga)
        if count > 0:
            raga_counts[raga] = count
    
    # Return the most frequent raga (or first one if tied)
    if raga_counts:
        most_common_raga = max(raga_counts.items(), key=lambda x: x[1])[0]
        return most_common_raga
    
    # If no known raga found, try to extract based on patterns
    # Look for capitalized words that might be ragas
    import re
    potential_ragas = re.findall(r'[A-Z][a-z]+', text)
    
    if potential_ragas:
        # Return the first potential raga
        return potential_ragas[0]

# Part 2: Parselmouth Feature Extraction (MFCCs, Spectral Centroid, Spectral Bandwidth)

def extract_parselmouth_features_with_viz(audio_path, create_viz=True):
    """Extract Parselmouth features and create visualization data"""
    features = {}
    viz_data = {}
    
    if not PARSELMOUTH_AVAILABLE:
        print("‚ö†Ô∏è Parselmouth not available")
        return features, viz_data
    
    try:
        # Load sound with Parselmouth
        sound = parselmouth.Sound(audio_path)
        print(f"üéµ Loaded: {sound.duration:.2f}s, {sound.sampling_frequency}Hz")
        
        # 1. MFCCs with visualization data
        try:
            mfcc = sound.to_mfcc(number_of_coefficients=13)
            mfcc_matrix = []
            
            for coeff_idx in range(13):
                mfcc_values = []
                for frame_idx in range(mfcc.get_number_of_frames()):
                    value = mfcc.get_value_in_frame(coeff_idx + 1, frame_idx + 1)
                    if not np.isnan(value) and not np.isinf(value):
                        mfcc_values.append(value)
                    else:
                        mfcc_values.append(0)
                
                mfcc_matrix.append(mfcc_values)
                
                # Extract statistics
                if mfcc_values:
                    features[f'mfcc_{coeff_idx}'] = np.mean(mfcc_values)
                    features[f'mfcc_{coeff_idx}_std'] = np.std(mfcc_values)
                    features[f'mfcc_{coeff_idx}_max'] = np.max(mfcc_values)
                    features[f'mfcc_{coeff_idx}_min'] = np.min(mfcc_values)
                else:
                    features[f'mfcc_{coeff_idx}'] = 0
                    features[f'mfcc_{coeff_idx}_std'] = 0
                    features[f'mfcc_{coeff_idx}_max'] = 0
                    features[f'mfcc_{coeff_idx}_min'] = 0
            
            # Store for visualization
            viz_data['mfcc_matrix'] = np.array(mfcc_matrix)
            viz_data['mfcc_time_axis'] = np.linspace(0, sound.duration, len(mfcc_matrix[0]))
            
            print(f"‚úÖ Extracted 13 MFCC coefficients")
            
        except Exception as e:
            print(f"‚ùå MFCC extraction failed: {e}")
            for i in range(13):
                features[f'mfcc_{i}'] = 0
                features[f'mfcc_{i}_std'] = 0
                features[f'mfcc_{i}_max'] = 0
                features[f'mfcc_{i}_min'] = 0
        
        # 2. Spectral features with time series data
        try:
            # Get spectrum at regular intervals for visualization
            num_frames = 100  # Number of frames for visualization
            time_points = np.linspace(0, sound.duration, num_frames)
            spectral_centroids = []
            spectral_bandwidths = []
            
            for t in time_points:
                try:
                    # Extract a short segment around this time point
                    start_time = max(0, t - 0.025)  # 50ms window
                    end_time = min(sound.duration, t + 0.025)
                    
                    segment = sound.extract_part(start_time, end_time)
                    spectrum = segment.to_spectrum()
                    
                    centroid = spectrum.get_centre_of_gravity()
                    bandwidth = spectrum.get_standard_deviation()
                    
                    if not np.isnan(centroid) and not np.isinf(centroid):
                        spectral_centroids.append(centroid)
                    else:
                        spectral_centroids.append(0)
                        
                    if not np.isnan(bandwidth) and not np.isinf(bandwidth):
                        spectral_bandwidths.append(bandwidth)
                    else:
                        spectral_bandwidths.append(0)
                        
                except:
                    spectral_centroids.append(0)
                    spectral_bandwidths.append(0)
            
            # Overall statistics
            if spectral_centroids:
                features['spectral_centroid'] = np.mean(spectral_centroids)
                features['spectral_centroid_std'] = np.std(spectral_centroids)
            else:
                features['spectral_centroid'] = 0
                features['spectral_centroid_std'] = 0
                
            if spectral_bandwidths:
                features['spectral_bandwidth'] = np.mean(spectral_bandwidths)
                features['spectral_bandwidth_std'] = np.std(spectral_bandwidths)
            else:
                features['spectral_bandwidth'] = 0
                features['spectral_bandwidth_std'] = 0
            
            # Store for visualization
            viz_data['spectral_centroid_series'] = spectral_centroids
            viz_data['spectral_bandwidth_series'] = spectral_bandwidths
            viz_data['spectral_time_axis'] = time_points
            
            print(f"‚úÖ Spectral features extracted")
            
        except Exception as e:
            print(f"‚ùå Spectral features failed: {e}")
            features['spectral_centroid'] = 0
            features['spectral_centroid_std'] = 0
            features['spectral_bandwidth'] = 0
            features['spectral_bandwidth_std'] = 0
        
        # 3. Additional Parselmouth features (Pitch, Formants)
        try:
            # Pitch features
            pitch = sound.to_pitch()
            pitch_values = []
            
            for i in range(pitch.get_number_of_frames()):
                value = pitch.get_value_in_frame(i + 1)
                if not np.isnan(value) and value > 0:
                    pitch_values.append(value)
            
            if pitch_values:
                features['pitch_mean'] = np.mean(pitch_values)
                features['pitch_std'] = np.std(pitch_values)
                features['pitch_range'] = np.max(pitch_values) - np.min(pitch_values)
                features['pitch_median'] = np.median(pitch_values)
            else:
                features['pitch_mean'] = 0
                features['pitch_std'] = 0
                features['pitch_range'] = 0
                features['pitch_median'] = 0
            
            print(f"‚úÖ Pitch features extracted")
            
        except Exception as e:
            print(f"‚ùå Pitch features failed: {e}")
            features['pitch_mean'] = 0
            features['pitch_std'] = 0
            features['pitch_range'] = 0
            features['pitch_median'] = 0
        
        # 4. Formant features
        try:
            formant = sound.to_formant_burg()
            f1_values = []
            f2_values = []
            f3_values = []
            
            for i in range(formant.get_number_of_frames()):
                time = formant.get_time_from_frame_number(i + 1)
                f1 = formant.get_value_at_time(1, time)
                f2 = formant.get_value_at_time(2, time)
                f3 = formant.get_value_at_time(3, time)
                
                if not np.isnan(f1) and f1 > 0:
                    f1_values.append(f1)
                if not np.isnan(f2) and f2 > 0:
                    f2_values.append(f2)
                if not np.isnan(f3) and f3 > 0:
                    f3_values.append(f3)
            
            # Formant statistics
            if f1_values:
                features['f1_mean'] = np.mean(f1_values)
                features['f1_std'] = np.std(f1_values)
            else:
                features['f1_mean'] = 0
                features['f1_std'] = 0
                
            if f2_values:
                features['f2_mean'] = np.mean(f2_values)
                features['f2_std'] = np.std(f2_values)
            else:
                features['f2_mean'] = 0
                features['f2_std'] = 0
                
            if f3_values:
                features['f3_mean'] = np.mean(f3_values)
                features['f3_std'] = np.std(f3_values)
            else:
                features['f3_mean'] = 0
                features['f3_std'] = 0
            
            print(f"‚úÖ Formant features extracted")
            
        except Exception as e:
            print(f"‚ùå Formant features failed: {e}")
            features['f1_mean'] = 0
            features['f1_std'] = 0
            features['f2_mean'] = 0
            features['f2_std'] = 0
            features['f3_mean'] = 0
            features['f3_std'] = 0
        
    except Exception as e:
        print(f"‚ùå Parselmouth processing failed: {e}")
    
    print(f"‚úÖ Parselmouth extracted {len(features)} features")
    return features, viz_data

print("‚úÖ Part 2: Parselmouth feature extraction functions ready!")

# Part 3: SpeechPy Feature Extraction (Chroma, Tempo, ZCR)

def extract_speechpy_features_with_viz(audio_path, create_viz=True):
    """Extract SpeechPy features with visualization data"""
    features = {}
    viz_data = {}
    
    if not SPEECHPY_AVAILABLE:
        print("‚ö†Ô∏è SpeechPy not available, using librosa fallback")
        if LIBROSA_AVAILABLE:
            return extract_librosa_fallback_features(audio_path, create_viz)
        else:
            return features, viz_data
    
    try:
        # Load audio using librosa (more reliable)
        if LIBROSA_AVAILABLE:
            y, sr = librosa.load(audio_path, sr=22050)
            signal = y
            sampling_rate = sr
        else:
            # Fallback to scipy
            if SCIPY_AVAILABLE:
                sampling_rate, signal = wavfile.read(audio_path)
                if len(signal.shape) > 1:  # Stereo to mono
                    signal = np.mean(signal, axis=1)
                signal = signal.astype(np.float32)
                signal = signal / np.max(np.abs(signal))  # Normalize
            else:
                print("‚ùå No audio loading library available")
                return features, viz_data
        
        print(f"üéµ SpeechPy loaded: {len(signal)/sampling_rate:.2f}s, {sampling_rate}Hz")
        
        # 1. Zero Crossing Rate using SpeechPy
        try:
            # SpeechPy ZCR - frame-based computation
            frame_length = 0.025  # 25ms frames
            frame_stride = 0.01   # 10ms stride
            
            frame_len_samples = int(frame_length * sampling_rate)
            frame_stride_samples = int(frame_stride * sampling_rate)
            
            # Calculate ZCR for each frame
            zcr_values = []
            time_axis = []
            
            for i in range(0, len(signal) - frame_len_samples, frame_stride_samples):
                frame = signal[i:i + frame_len_samples]
                
                # Calculate zero crossings
                zero_crossings = np.sum(np.abs(np.diff(np.sign(frame)))) / 2
                zcr = zero_crossings / len(frame)
                
                zcr_values.append(zcr)
                time_axis.append(i / sampling_rate)
            
            zcr_values = np.array(zcr_values)
            time_axis = np.array(time_axis)
            
            # Extract statistics
            features['zcr_mean'] = np.mean(zcr_values)
            features['zcr_std'] = np.std(zcr_values)
            features['zcr_max'] = np.max(zcr_values)
            features['zcr_min'] = np.min(zcr_values)
            features['zcr_median'] = np.median(zcr_values)
            
            # Store for visualization
            viz_data['zcr_series'] = zcr_values
            viz_data['zcr_time_axis'] = time_axis
            
            print(f"‚úÖ ZCR extracted using SpeechPy")
            
        except Exception as e:
            print(f"‚ùå SpeechPy ZCR extraction failed: {e}")
            features.update({'zcr_mean': 0, 'zcr_std': 0, 'zcr_max': 0, 'zcr_min': 0, 'zcr_median': 0})
        
        # 2. Chroma features using SpeechPy + librosa
        try:
            if LIBROSA_AVAILABLE:
                # Use librosa for chroma as it's more reliable
                chroma = librosa.feature.chroma_stft(y=signal, sr=sampling_rate)
                
                for i in range(12):
                    chroma_values = chroma[i, :]
                    features[f'chroma_{i+1}_mean'] = np.mean(chroma_values)
                    features[f'chroma_{i+1}_std'] = np.std(chroma_values)
                    features[f'chroma_{i+1}_max'] = np.max(chroma_values)
                    features[f'chroma_{i+1}_min'] = np.min(chroma_values)
                
                # Store for visualization
                viz_data['chroma_matrix'] = chroma
                viz_data['chroma_time_axis'] = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sampling_rate)
                
                print(f"‚úÖ Extracted 12 chroma features using SpeechPy + librosa")
            else:
                # Basic chroma computation without librosa
                print("‚ö†Ô∏è Librosa not available, creating basic chroma features")
                for i in range(12):
                    features[f'chroma_{i+1}_mean'] = 0
                    features[f'chroma_{i+1}_std'] = 0
                    features[f'chroma_{i+1}_max'] = 0
                    features[f'chroma_{i+1}_min'] = 0
            
        except Exception as e:
            print(f"‚ùå Chroma extraction failed: {e}")
            for i in range(12):
                features[f'chroma_{i+1}_mean'] = 0
                features[f'chroma_{i+1}_std'] = 0
                features[f'chroma_{i+1}_max'] = 0
                features[f'chroma_{i+1}_min'] = 0
        
        # 3. Tempo estimation using SpeechPy + signal processing
        try:
            # Calculate energy-based tempo estimation
            frame_length = 0.05  # 50ms frames for tempo
            frame_stride = 0.025  # 25ms stride
            
            frame_len_samples = int(frame_length * sampling_rate)
            frame_stride_samples = int(frame_stride * sampling_rate)
            
            # Calculate frame energy
            energy_values = []
            tempo_time_axis = []
            
            for i in range(0, len(signal) - frame_len_samples, frame_stride_samples):
                frame = signal[i:i + frame_len_samples]
                energy = np.sum(frame ** 2)
                energy_values.append(energy)
                tempo_time_axis.append(i / sampling_rate)
            
            energy_values = np.array(energy_values)
            tempo_time_axis = np.array(tempo_time_axis)
            
            # Find peaks in energy for tempo estimation
            if SCIPY_AVAILABLE:
                peaks, _ = find_peaks(energy_values, height=np.mean(energy_values), distance=int(0.3/frame_stride))
            else:
                # Manual peak detection
                peaks = []
                threshold = np.mean(energy_values)
                min_distance = int(0.3/frame_stride)  # Minimum 300ms between beats
                
                for i in range(1, len(energy_values)-1):
                    if (energy_values[i] > energy_values[i-1] and 
                        energy_values[i] > energy_values[i+1] and
                        energy_values[i] > threshold):
                        
                        # Check minimum distance from previous peaks
                        if not peaks or (i - peaks[-1]) >= min_distance:
                            peaks.append(i)
                
                peaks = np.array(peaks)
            
            # Estimate tempo from peaks
            if len(peaks) > 1:
                peak_intervals = np.diff(peaks) * frame_stride
                avg_interval = np.mean(peak_intervals)
                estimated_tempo = 60 / avg_interval if avg_interval > 0 else 120
                
                # Clamp tempo to reasonable range
                estimated_tempo = min(max(estimated_tempo, 60), 200)
                
                features['tempo_estimated'] = estimated_tempo
                features['tempo_stability'] = 1 / (np.std(peak_intervals) + 1e-7)
                features['tempo_confidence'] = len(peaks) / len(energy_values)
            else:
                features['tempo_estimated'] = 120
                features['tempo_stability'] = 0
                features['tempo_confidence'] = 0
            
            # Store for visualization
            viz_data['tempo_curve'] = energy_values
            viz_data['tempo_peaks'] = peaks
            viz_data['tempo_time_axis'] = tempo_time_axis
            viz_data['estimated_tempo'] = features['tempo_estimated']
            
            print(f"‚úÖ Tempo estimated using SpeechPy: {features['tempo_estimated']:.1f} BPM")
            
        except Exception as e:
            print(f"‚ùå Tempo estimation failed: {e}")
            features['tempo_estimated'] = 120
            features['tempo_stability'] = 0
            features['tempo_confidence'] = 0
        
        # 4. Additional SpeechPy features
        try:
            # MFCC features using SpeechPy (as backup to Parselmouth)
            num_cepstral = 13
            
            # Use SpeechPy's MFCC implementation
            mfcc_features = speechpy.feature.mfcc(signal, sampling_rate, 
                                                 frame_length=0.025, frame_stride=0.01,
                                                 num_cepstral=num_cepstral, num_filters=26)
            
            for i in range(num_cepstral):
                if i < mfcc_features.shape[1]:
                    mfcc_values = mfcc_features[:, i]
                    features[f'speechpy_mfcc_{i}_mean'] = np.mean(mfcc_values)
                    features[f'speechpy_mfcc_{i}_std'] = np.std(mfcc_values)
                else:
                    features[f'speechpy_mfcc_{i}_mean'] = 0
                    features[f'speechpy_mfcc_{i}_std'] = 0
            
            print(f"‚úÖ Additional {num_cepstral} MFCC features from SpeechPy")
            
        except Exception as e:
            print(f"‚ùå SpeechPy MFCC extraction failed: {e}")
            for i in range(13):
                features[f'speechpy_mfcc_{i}_mean'] = 0
                features[f'speechpy_mfcc_{i}_std'] = 0
        
        # 5. Spectral features using SpeechPy
        try:
            # Spectral centroid and bandwidth using SpeechPy
            frame_length = 0.025
            frame_stride = 0.01
            
            # Extract power spectral density features
            stft = np.abs(librosa.stft(signal)) if LIBROSA_AVAILABLE else None
            
            if stft is not None:
                # Spectral centroid
                spectral_centroids = librosa.feature.spectral_centroid(S=stft, sr=sampling_rate)[0]
                features['speechpy_spectral_centroid'] = np.mean(spectral_centroids)
                features['speechpy_spectral_centroid_std'] = np.std(spectral_centroids)
                
                # Spectral bandwidth
                spectral_bandwidth = librosa.feature.spectral_bandwidth(S=stft, sr=sampling_rate)[0]
                features['speechpy_spectral_bandwidth'] = np.mean(spectral_bandwidth)
                features['speechpy_spectral_bandwidth_std'] = np.std(spectral_bandwidth)
                
                print(f"‚úÖ Spectral features extracted using SpeechPy + librosa")
            else:
                features['speechpy_spectral_centroid'] = 0
                features['speechpy_spectral_centroid_std'] = 0
                features['speechpy_spectral_bandwidth'] = 0
                features['speechpy_spectral_bandwidth_std'] = 0
            
        except Exception as e:
            print(f"‚ùå SpeechPy spectral features failed: {e}")
            features['speechpy_spectral_centroid'] = 0
            features['speechpy_spectral_centroid_std'] = 0
            features['speechpy_spectral_bandwidth'] = 0
            features['speechpy_spectral_bandwidth_std'] = 0
        
    except Exception as e:
        print(f"‚ùå SpeechPy processing failed: {e}")
        if LIBROSA_AVAILABLE:
            return extract_librosa_fallback_features(audio_path, create_viz)
    
    print(f"‚úÖ SpeechPy extracted {len(features)} features")
    return features, viz_data

def extract_librosa_fallback_features(audio_path, create_viz=True):
    """Fallback feature extraction using librosa"""
    features = {}
    viz_data = {}
    
    if not LIBROSA_AVAILABLE:
        return features, viz_data
    
    try:
        print("üîÑ Using librosa fallback for feature extraction")
        y, sr = librosa.load(audio_path, sr=22050)
        
        # 1. Zero Crossing Rate
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        features['zcr_mean'] = np.mean(zcr)
        features['zcr_std'] = np.std(zcr)
        features['zcr_max'] = np.max(zcr)
        features['zcr_min'] = np.min(zcr)
        features['zcr_median'] = np.median(zcr)
        
        # Store for visualization
        viz_data['zcr_series'] = zcr
        viz_data['zcr_time_axis'] = librosa.frames_to_time(np.arange(len(zcr)), sr=sr)
        
        # 2. Chroma features
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        for i in range(12):
            features[f'chroma_{i+1}_mean'] = np.mean(chroma[i])
            features[f'chroma_{i+1}_std'] = np.std(chroma[i])
            features[f'chroma_{i+1}_max'] = np.max(chroma[i])
            features[f'chroma_{i+1}_min'] = np.min(chroma[i])
        
        # Store for visualization
        viz_data['chroma_matrix'] = chroma
        viz_data['chroma_time_axis'] = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sr)
        
        # 3. Tempo estimation
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
        features['tempo_estimated'] = tempo
        features['tempo_stability'] = 1.0  # Default stability
        features['tempo_confidence'] = 1.0
        
        # Store for visualization
        onset_env = librosa.onset.onset_strength(y=y, sr=sr)
        viz_data['tempo_curve'] = onset_env
        viz_data['tempo_peaks'] = beats
        viz_data['tempo_time_axis'] = librosa.frames_to_time(np.arange(len(onset_env)), sr=sr)
        viz_data['estimated_tempo'] = tempo
        
        print("‚úÖ Librosa fallback features extracted")
        
    except Exception as e:
        print(f"‚ùå Librosa fallback failed: {e}")
    
    return features, viz_data

print("‚úÖ Part 3: SpeechPy feature extraction functions ready!")
print("üéµ Features: Chroma (12), Zero Crossing Rate, Tempo Estimation, Additional MFCCs")

# Part 4: Updated Feature Extraction and Visualizations with SpeechPy

def extract_all_features_with_viz(audio_path):
    """Extract all features using Parselmouth + SpeechPy and create visualizations"""
    print(f"\nüéµ Processing: {os.path.basename(audio_path)}")
    print("-" * 60)
    
    # Extract from both systems with visualization data
    parselmouth_features, parselmouth_viz = extract_parselmouth_features_with_viz(audio_path)
    speechpy_features, speechpy_viz = extract_speechpy_features_with_viz(audio_path)
    
    # Combine features
    all_features = {**parselmouth_features, **speechpy_features}
    
    # Combine visualization data
    all_viz_data = {**parselmouth_viz, **speechpy_viz}
    
    # Create specific visualizations
    if all_viz_data:
        file_name = os.path.splitext(os.path.basename(audio_path))[0]
        create_specific_feature_visualizations(all_viz_data, file_name, file_name)
    
    print(f"\n‚úÖ Total features extracted: {len(all_features)}")
    print(f"   üìä Parselmouth: {len(parselmouth_features)}")
    print(f"   üìä SpeechPy: {len(speechpy_features)}")
    print("=" * 60)
    
    return all_features

def create_specific_feature_visualizations(viz_data, file_name, save_prefix="sample"):
    """Create specific visualizations as requested"""
    
    print(f"üé® Creating specific visualizations for {file_name}...")
    
    # Create a large figure with subplots
    fig = plt.figure(figsize=(20, 24))
    
    # 1. MFCCs ‚Üí Heatmap
    if 'mfcc_matrix' in viz_data:
        plt.subplot(4, 2, 1)
        mfcc_data = viz_data['mfcc_matrix']
        
        im = plt.imshow(mfcc_data, aspect='auto', origin='lower', cmap='viridis')
        plt.colorbar(im, shrink=0.8)
        plt.title('MFCC Heatmap (Parselmouth)', fontsize=14, fontweight='bold')
        plt.xlabel('Time Frames')
        plt.ylabel('MFCC Coefficients')
        plt.yticks(range(13), [f'MFCC {i}' for i in range(13)])
    
    # 2. MFCCs ‚Üí Line Plot
    if 'mfcc_matrix' in viz_data:
        plt.subplot(4, 2, 2)
        mfcc_data = viz_data['mfcc_matrix']
        time_axis = viz_data['mfcc_time_axis']
        
        # Plot first 4 MFCC coefficients as lines
        colors = ['red', 'blue', 'green', 'orange']
        for i in range(min(4, len(mfcc_data))):
            plt.plot(time_axis, mfcc_data[i], label=f'MFCC {i}', 
                    alpha=0.8, color=colors[i], linewidth=2)
        
        plt.title('MFCC Line Plot (First 4 Coefficients)', fontsize=14, fontweight='bold')
        plt.xlabel('Time (s)')
        plt.ylabel('MFCC Value')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # 3. Spectral Centroid ‚Üí Line Plot
    if 'spectral_centroid_series' in viz_data:
        plt.subplot(4, 2, 3)
        centroid_data = viz_data['spectral_centroid_series']
        time_axis = viz_data['spectral_time_axis']
        
        plt.plot(time_axis, centroid_data, 'b-', linewidth=2, alpha=0.8)
        plt.title('Spectral Centroid Over Time (Parselmouth)', fontsize=14, fontweight='bold')
        plt.xlabel('Time (s)')
        plt.ylabel('Frequency (Hz)')
        plt.grid(True, alpha=0.3)
        
        # Add mean line
        mean_centroid = np.mean(centroid_data)
        plt.axhline(y=mean_centroid, color='r', linestyle='--', alpha=0.7, 
                   label=f'Mean: {mean_centroid:.1f} Hz')
        plt.legend()
    
    # 4. Spectral Bandwidth ‚Üí Line Plot
    if 'spectral_bandwidth_series' in viz_data:
        plt.subplot(4, 2, 4)
        bandwidth_data = viz_data['spectral_bandwidth_series']
        time_axis = viz_data['spectral_time_axis']
        
        plt.plot(time_axis, bandwidth_data, 'g-', linewidth=2, alpha=0.8)
        plt.title('Spectral Bandwidth Over Time (Parselmouth)', fontsize=14, fontweight='bold')
        plt.xlabel('Time (s)')
        plt.ylabel('Bandwidth (Hz)')
        plt.grid(True, alpha=0.3)
        
        # Add mean line
        mean_bandwidth = np.mean(bandwidth_data)
        plt.axhline(y=mean_bandwidth, color='r', linestyle='--', alpha=0.7,
                   label=f'Mean: {mean_bandwidth:.1f} Hz')
        plt.legend()
    
    # 5. Chroma ‚Üí Heatmap
    if 'chroma_matrix' in viz_data:
        plt.subplot(4, 2, 5)
        chroma_data = viz_data['chroma_matrix']
        
        im = plt.imshow(chroma_data, aspect='auto', origin='lower', cmap='Reds')
        plt.colorbar(im, shrink=0.8)
        plt.title('Chroma Heatmap (SpeechPy)', fontsize=14, fontweight='bold')
        plt.xlabel('Time Frames')
        plt.ylabel('Chroma Features')
        chroma_labels = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        plt.yticks(range(min(12, len(chroma_data))), chroma_labels[:len(chroma_data)])
    
    # 6. Tempo Estimation ‚Üí Bar Graph
    if 'estimated_tempo' in viz_data:
        plt.subplot(4, 2, 6)
        tempo = viz_data['estimated_tempo']
        
        # Create tempo ranges for comparison
        tempo_ranges = ['Very Slow\n(60-80)', 'Slow\n(80-100)', 'Moderate\n(100-120)', 
                       'Fast\n(120-140)', 'Very Fast\n(140-200)']
        range_values = [70, 90, 110, 130, 170]
        range_colors = ['blue', 'green', 'orange', 'red', 'purple']
        
        # Determine which range the tempo falls into
        current_range = 2  # Default to moderate
        if tempo < 80:
            current_range = 0
        elif tempo < 100:
            current_range = 1
        elif tempo < 120:
            current_range = 2
        elif tempo < 140:
            current_range = 3
        else:
            current_range = 4
        
        colors = ['lightgray'] * 5
        colors[current_range] = range_colors[current_range]
        
        bars = plt.bar(range(5), [tempo if i == current_range else range_values[i] for i in range(5)], 
                      color=colors, alpha=0.7)
        
        plt.title(f'Tempo Classification (SpeechPy): {tempo:.1f} BPM', fontsize=14, fontweight='bold')
        plt.xlabel('Tempo Categories')
        plt.ylabel('BPM')
        plt.xticks(range(5), tempo_ranges)
        
        # Highlight current tempo
        bars[current_range].set_edgecolor('black')
        bars[current_range].set_linewidth(3)
        
        # Add tempo value as text
        plt.text(current_range, tempo + 5, f'{tempo:.1f}', 
                ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    # 7. Tempo Curve
    if 'tempo_curve' in viz_data:
        plt.subplot(4, 2, 7)
        tempo_curve = viz_data['tempo_curve']
        time_axis = viz_data['tempo_time_axis']
        
        plt.plot(time_axis, tempo_curve, 'purple', linewidth=1.5, alpha=0.8)
        plt.title('Tempo Curve - Energy Based (SpeechPy)', fontsize=14, fontweight='bold')
        plt.xlabel('Time (s)')
        plt.ylabel('Energy/Onset Strength')
        plt.grid(True, alpha=0.3)
        
        # Mark detected peaks if available
        if 'tempo_peaks' in viz_data:
            peaks = viz_data['tempo_peaks']
            if len(peaks) > 0 and len(time_axis) > max(peaks):
                peak_times = time_axis[peaks]
                peak_values = tempo_curve[peaks]
                plt.scatter(peak_times, peak_values, color='red', s=30, alpha=0.8, 
                           label='Detected Beats', zorder=5)
                plt.legend()
    
    # 8. Zero Crossing Rate ‚Üí Line Plot
    if 'zcr_series' in viz_data:
        plt.subplot(4, 2, 8)
        zcr_data = viz_data['zcr_series']
        time_axis = viz_data['zcr_time_axis']
        
        plt.plot(time_axis, zcr_data, 'orange', linewidth=1.5, alpha=0.8)
        plt.title('Zero Crossing Rate Over Time (SpeechPy)', fontsize=14, fontweight='bold')
        plt.xlabel('Time (s)')
        plt.ylabel('ZCR')
        plt.grid(True, alpha=0.3)
        
        # Add mean and std lines
        mean_zcr = np.mean(zcr_data)
        std_zcr = np.std(zcr_data)
        plt.axhline(y=mean_zcr, color='r', linestyle='--', alpha=0.7, 
                   label=f'Mean: {mean_zcr:.4f}')
        plt.axhline(y=mean_zcr + std_zcr, color='g', linestyle=':', alpha=0.7, 
                   label=f'+1œÉ: {mean_zcr + std_zcr:.4f}')
        plt.axhline(y=mean_zcr - std_zcr, color='g', linestyle=':', alpha=0.7, 
                   label=f'-1œÉ: {mean_zcr - std_zcr:.4f}')
        plt.legend(fontsize=8)
    
    plt.suptitle(f'Feature Visualizations: {file_name} (Parselmouth + SpeechPy)', 
                fontsize=18, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.subplots_adjust(top=0.95)
    
    # Save the comprehensive visualization
    save_path = f'results/graphs/visualizations/{save_prefix}_feature_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create ZCR Histogram separately
    if 'zcr_series' in viz_data:
        plt.figure(figsize=(10, 6))
        zcr_data = viz_data['zcr_series']
        
        plt.hist(zcr_data, bins=30, alpha=0.7, color='orange', edgecolor='black')
        plt.title('Zero Crossing Rate Distribution (SpeechPy)', fontsize=14, fontweight='bold')
        plt.xlabel('ZCR Value')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        
        # Add statistics
        mean_zcr = np.mean(zcr_data)
        std_zcr = np.std(zcr_data)
        median_zcr = np.median(zcr_data)
        
        plt.axvline(x=mean_zcr, color='r', linestyle='--', linewidth=2, 
                   label=f'Mean: {mean_zcr:.4f}')
        plt.axvline(x=median_zcr, color='b', linestyle='-', linewidth=2, 
                   label=f'Median: {median_zcr:.4f}')
        plt.axvline(x=mean_zcr + std_zcr, color='g', linestyle=':', linewidth=2, 
                   label=f'+1œÉ: {mean_zcr + std_zcr:.4f}')
        plt.axvline(x=mean_zcr - std_zcr, color='g', linestyle=':', linewidth=2, 
                   label=f'-1œÉ: {mean_zcr - std_zcr:.4f}')
        
        # Add text box with statistics
        stats_text = f'Statistics:\nMean: {mean_zcr:.4f}\nStd: {std_zcr:.4f}\nMedian: {median_zcr:.4f}\nMin: {np.min(zcr_data):.4f}\nMax: {np.max(zcr_data):.4f}'
        plt.text(0.7, 0.95, stats_text, transform=plt.gca().transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.legend()
        plt.tight_layout()
        zcr_hist_path = f'results/graphs/visualizations/{save_prefix}_zcr_histogram.png'
        plt.savefig(zcr_hist_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ ZCR histogram saved: {zcr_hist_path}")
    
    # Create Chroma Analysis separately
    if 'chroma_matrix' in viz_data:
        plt.figure(figsize=(12, 8))
        chroma_data = viz_data['chroma_matrix']
        
        # Create violin plots for each chroma feature
        chroma_df = pd.DataFrame(chroma_data.T, columns=['C', 'C#', 'D', 'D#', 'E', 'F', 
                                                        'F#', 'G', 'G#', 'A', 'A#', 'B'])
        
        # Melt data for violin plot
        chroma_melted = pd.melt(chroma_df, var_name='Note', value_name='Chroma_Value')
        
        sns.violinplot(data=chroma_melted, x='Note', y='Chroma_Value', palette='Set3')
        plt.title('Chroma Features Distribution by Musical Note (SpeechPy)', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Musical Notes', fontsize=12)
        plt.ylabel('Chroma Values', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        chroma_analysis_path = f'results/graphs/visualizations/{save_prefix}_chroma_analysis.png'
        plt.savefig(chroma_analysis_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Chroma analysis saved: {chroma_analysis_path}")
    
    # Create MFCC Analysis separately
    if 'mfcc_matrix' in viz_data:
        plt.figure(figsize=(14, 8))
        mfcc_data = viz_data['mfcc_matrix']
        
        # Create boxplots for MFCC coefficients
        mfcc_df = pd.DataFrame(mfcc_data.T, columns=[f'MFCC_{i}' for i in range(mfcc_data.shape[0])])
        
        # Melt data for boxplot
        mfcc_melted = pd.melt(mfcc_df, var_name='MFCC_Coefficient', value_name='MFCC_Value')
        
        sns.boxplot(data=mfcc_melted, x='MFCC_Coefficient', y='MFCC_Value', palette='viridis')
        plt.title('MFCC Coefficients Distribution (Parselmouth)', fontsize=14, fontweight='bold')
        plt.xlabel('MFCC Coefficients', fontsize=12)
        plt.ylabel('MFCC Values', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        mfcc_analysis_path = f'results/graphs/visualizations/{save_prefix}_mfcc_analysis.png'
        plt.savefig(mfcc_analysis_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ MFCC analysis saved: {mfcc_analysis_path}")
    
    print(f"‚úÖ All feature visualizations saved!")
    print(f"üìä Main visualization: {save_path}")
    return save_path

def create_feature_comparison_plot(all_features, file_name):
    """Create a comparison plot of different feature types"""
    
    plt.figure(figsize=(16, 10))
    
    # Separate features by type
    mfcc_features = {k: v for k, v in all_features.items() if 'mfcc' in k.lower() and 'mean' in k}
    chroma_features = {k: v for k, v in all_features.items() if 'chroma' in k.lower() and 'mean' in k}
    spectral_features = {k: v for k, v in all_features.items() if 'spectral' in k.lower()}
    temporal_features = {k: v for k, v in all_features.items() if any(term in k.lower() for term in ['zcr', 'tempo'])}
    
    # Create subplots for different feature types
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # MFCC Features
    if mfcc_features:
        ax = axes[0, 0]
        features_names = list(mfcc_features.keys())[:13]  # First 13 MFCCs
        features_values = [mfcc_features[name] for name in features_names]
        
        bars = ax.bar(range(len(features_names)), features_values, color='skyblue', alpha=0.7)
        ax.set_title('MFCC Features', fontsize=14, fontweight='bold')
        ax.set_xlabel('MFCC Coefficients')
        ax.set_ylabel('Values')
        ax.set_xticks(range(len(features_names)))
        ax.set_xticklabels([f'MFCC {i}' for i in range(len(features_names))], rotation=45)
        ax.grid(True, alpha=0.3)
    
    # Chroma Features
    if chroma_features:
        ax = axes[0, 1]
        chroma_names = sorted([k for k in chroma_features.keys() if 'chroma' in k])[:12]
        chroma_values = [chroma_features[name] for name in chroma_names]
        
        note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        bars = ax.bar(range(len(chroma_values)), chroma_values, color='lightcoral', alpha=0.7)
        ax.set_title('Chroma Features', fontsize=14, fontweight='bold')
        ax.set_xlabel('Musical Notes')
        ax.set_ylabel('Values')
        ax.set_xticks(range(len(chroma_values)))
        ax.set_xticklabels(note_names[:len(chroma_values)], rotation=45)
        ax.grid(True, alpha=0.3)
    
    # Spectral Features
    if spectral_features:
        ax = axes[1, 0]
        spec_names = list(spectral_features.keys())
        spec_values = list(spectral_features.values())
        
        bars = ax.bar(range(len(spec_names)), spec_values, color='lightgreen', alpha=0.7)
        ax.set_title('Spectral Features', fontsize=14, fontweight='bold')
        ax.set_xlabel('Feature Types')
        ax.set_ylabel('Values')
        ax.set_xticks(range(len(spec_names)))
        ax.set_xticklabels([name.replace('_', ' ').title() for name in spec_names], rotation=45)
        ax.grid(True, alpha=0.3)
    
    # Temporal Features
    if temporal_features:
        ax = axes[1, 1]
        temp_names = list(temporal_features.keys())
        temp_values = list(temporal_features.values())
        
        bars = ax.bar(range(len(temp_names)), temp_values, color='gold', alpha=0.7)
        ax.set_title('Temporal Features', fontsize=14, fontweight='bold')
        ax.set_xlabel('Feature Types')
        ax.set_ylabel('Values')
        ax.set_xticks(range(len(temp_names)))
        ax.set_xticklabels([name.replace('_', ' ').title() for name in temp_names], rotation=45)
        ax.grid(True, alpha=0.3)
    
    plt.suptitle(f'Feature Comparison: {file_name}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    comparison_path = f'results/graphs/visualizations/{file_name}_feature_comparison.png'
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Feature comparison saved: {comparison_path}")

print("‚úÖ Part 4: Updated feature extraction and visualizations with SpeechPy ready!")
print("üé® Visualizations include:")
print("   üìä Main feature analysis plot")
print("   üìä ZCR histogram with statistics")
print("   üìä Chroma analysis by musical notes")
print("   üìä MFCC coefficients distribution")
print("   üìä Feature comparison by types")

# Fixed Part 5: Enhanced Data Preparation (Fixed for Concatenated Raga Names)

def load_datasets():
    """Load both datasets with proper path handling"""
    print("üìÅ Loading datasets...")
    
    # Use the exact paths provided
    dataset1_path = "data/Dataset.csv"
    dataset2_path = "data/Final_dataset_s.csv"
    
    # Check if files exist
    if not os.path.exists(dataset1_path):
        print(f"‚ùå {dataset1_path} not found")
        return None, None
    
    if not os.path.exists(dataset2_path):
        print(f"‚ùå {dataset2_path} not found") 
        return None, None
    
    try:
        df1 = pd.read_csv(dataset1_path)
        df2 = pd.read_csv(dataset2_path)
        
        print(f"‚úÖ Dataset.csv: {df1.shape}")
        print(f"   Columns: {list(df1.columns)}")
        print(f"‚úÖ Final_dataset_s.csv: {df2.shape}")
        print(f"   Columns: {list(df2.columns)}")
        
        return df1, df2
        
    except Exception as e:
        print(f"‚ùå Error loading datasets: {e}")
        return None, None

def create_standardized_feature_mapping():
    """Create standardized feature mapping for better alignment"""
    
    # Standard feature mapping to align both datasets
    feature_mapping = {
        # MFCC mappings (Dataset.csv uses mfcc0-mfcc18, Final_dataset_s.csv uses mfcc1-mfcc13)
        'mfcc0': 'mfcc_0', 'mfcc1': 'mfcc_1', 'mfcc2': 'mfcc_2', 'mfcc3': 'mfcc_3',
        'mfcc4': 'mfcc_4', 'mfcc5': 'mfcc_5', 'mfcc6': 'mfcc_6', 'mfcc7': 'mfcc_7',
        'mfcc8': 'mfcc_8', 'mfcc9': 'mfcc_9', 'mfcc10': 'mfcc_10', 'mfcc11': 'mfcc_11',
        'mfcc12': 'mfcc_12', 'mfcc13': 'mfcc_13', 'mfcc14': 'mfcc_14', 'mfcc15': 'mfcc_15',
        'mfcc16': 'mfcc_16', 'mfcc17': 'mfcc_17', 'mfcc18': 'mfcc_18',
        
        # Reverse mapping
        'mfcc_0': 'mfcc0', 'mfcc_1': 'mfcc1', 'mfcc_2': 'mfcc2', 'mfcc_3': 'mfcc3',
        'mfcc_4': 'mfcc4', 'mfcc_5': 'mfcc5', 'mfcc_6': 'mfcc6', 'mfcc_7': 'mfcc7',
        'mfcc_8': 'mfcc8', 'mfcc_9': 'mfcc9', 'mfcc_10': 'mfcc10', 'mfcc_11': 'mfcc11',
        'mfcc_12': 'mfcc12', 'mfcc_13': 'mfcc13',
        
        # Chroma mappings
        'chroma_stft': 'chroma_1',
        'chroma1': 'chroma_1', 'chroma2': 'chroma_2', 'chroma3': 'chroma_3',
        'chroma4': 'chroma_4', 'chroma5': 'chroma_5', 'chroma6': 'chroma_6',
        'chroma7': 'chroma_7', 'chroma8': 'chroma_8', 'chroma9': 'chroma_9',
        'chroma10': 'chroma_10', 'chroma11': 'chroma_11', 'chroma12': 'chroma_12',
        
        # Spectral mappings
        'spec_cent': 'spectral_centroid',
        'spec_bw': 'spectral_bandwidth',
        'spectral_centroid': 'spectral_centroid',
        'spectral_bandwidth': 'spectral_bandwidth',
        
        # Temporal mappings
        'zero_crossing_rate': 'zcr_mean',
        'zcr': 'zcr_mean',
        'tempo': 'tempo_estimated',
        
        # Target mapping
        'raga': 'Raga',
        'RAGA': 'Raga'
    }
    
    return feature_mapping

def extract_individual_ragas(concatenated_string):
    """Extract individual raga names from concatenated string"""
    
    # Known raga names from the dataset
    known_ragas = [
        'Bhairav', 'Marwa', 'Malkauns', 'Hindol', 'Bilawal', 'Todi', 
        'Chandrakauns', 'Madhuvanti', 'Shree', 'Khamaj', 'Yaman', 
        'Kafi', 'Bhairavi', 'Kedar'
    ]
    
    # Convert to string and clean
    text = str(concatenated_string).strip()
    
    # If it's already a single raga name, return it
    if text in known_ragas:
        return text
    
    # Count occurrences of each raga in the concatenated string
    raga_counts = {}
    for raga in known_ragas:
        count = text.count(raga)
        if count > 0:
            raga_counts[raga] = count
    
    # Return the most frequent raga (or first one if tied)
    if raga_counts:
        most_common_raga = max(raga_counts.items(), key=lambda x: x[1])[0]
        return most_common_raga
    
    # If no known raga found, try to extract based on patterns
    # Look for capitalized words that might be ragas
    import re
    potential_ragas = re.findall(r'[A-Z][a-z]+', text)
    
    if potential_ragas:
        # Return the first potential raga
        return potential_ragas[0]
    
    # Last resort: return the original text (cleaned)
    return text

def clean_raga_column(df, raga_column_name):
    """Clean the raga column by extracting individual raga names"""
    
    print(f"üßπ Cleaning raga column: {raga_column_name}")
    
    # Sample the data to see what we're dealing with
    sample_values = df[raga_column_name].head(10).tolist()
    print(f"üìù Sample values: {sample_values[:3]}")
    
    # Apply the extraction function
    cleaned_ragas = df[raga_column_name].apply(extract_individual_ragas)
    
    # Show the results
    unique_ragas = cleaned_ragas.unique()
    print(f"‚úÖ Extracted {len(unique_ragas)} unique ragas: {list(unique_ragas)}")
    
    # Show the distribution
    raga_counts = cleaned_ragas.value_counts()
    print(f"üìä Raga distribution:")
    for raga, count in raga_counts.head(10).items():
        print(f"   {raga}: {count} samples")
    
    return cleaned_ragas

def prepare_enhanced_dataset(df1, df2):
    """Enhanced dataset preparation with concatenated raga name handling"""
    print("\nüîß Enhanced dataset preparation for concatenated raga names...")
    
    # Get feature mapping
    feature_mapping = create_standardized_feature_mapping()
    
    # Clean and standardize datasets
    df1_clean = df1.copy()
    df2_clean = df2.copy()
    
    # Remove unnecessary columns
    columns_to_remove = ['Unnamed: 0', 'filename', 'Age', 'Gender', 'Mental_Condition', 
                        'Severity', 'Improvement_Score', 'Listening_Time']
    
    for col in columns_to_remove:
        if col in df1_clean.columns:
            df1_clean = df1_clean.drop(col, axis=1)
        if col in df2_clean.columns:
            df2_clean = df2_clean.drop(col, axis=1)
    
    # Apply feature mapping to both datasets
    df1_clean = df1_clean.rename(columns=feature_mapping)
    df2_clean = df2_clean.rename(columns=feature_mapping)
    
    # Standardize target column
    for col in ['raga', 'Raga', 'RAGA']:
        if col in df1_clean.columns:
            df1_clean = df1_clean.rename(columns={col: 'Raga'})
        if col in df2_clean.columns:
            df2_clean = df2_clean.rename(columns={col: 'Raga'})
    
    # Clean the Raga columns (handle concatenated names)
    if 'Raga' in df1_clean.columns:
        df1_clean['Raga'] = clean_raga_column(df1_clean, 'Raga')
    
    if 'Raga' in df2_clean.columns:
        df2_clean['Raga'] = clean_raga_column(df2_clean, 'Raga')
    
    # Define priority features for raga classification
    priority_features = [
        # MFCCs (most important for audio classification)
        'mfcc0', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6',
        'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12',
        'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6',
        'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13',
        
        # Chroma (crucial for ragas - musical notes)
        'chroma_stft', 'chroma1', 'chroma2', 'chroma3', 'chroma4', 'chroma5', 'chroma6',
        'chroma7', 'chroma8', 'chroma9', 'chroma10', 'chroma11', 'chroma12',
        'chroma_1', 'chroma_2', 'chroma_3', 'chroma_4', 'chroma_5', 'chroma_6',
        'chroma_7', 'chroma_8', 'chroma_9', 'chroma_10', 'chroma_11', 'chroma_12',
        
        # Spectral (important for timbre)
        'spec_cent', 'spec_bw', 'spectral_centroid', 'spectral_bandwidth',
        
        # Temporal (rhythm patterns)
        'zero_crossing_rate', 'tempo', 'zcr_mean', 'tempo_estimated',
        
        # Additional features
        'rmse'
    ]
    
    # Find available features in each dataset
    df1_available = [col for col in priority_features if col in df1_clean.columns]
    df2_available = [col for col in priority_features if col in df2_clean.columns]
    
    # Add target
    if 'Raga' in df1_clean.columns:
        df1_available.append('Raga')
    if 'Raga' in df2_clean.columns:
        df2_available.append('Raga')
    
    print(f"üìä Dataset.csv priority features: {len(df1_available)-1 if 'Raga' in df1_available else len(df1_available)}")
    print(f"üìä Final_dataset_s.csv priority features: {len(df2_available)-1 if 'Raga' in df2_available else len(df2_available)}")
    
    # Create filtered datasets
    df1_filtered = df1_clean[df1_available].copy() if df1_available else pd.DataFrame()
    df2_filtered = df2_clean[df2_available].copy() if df2_available else pd.DataFrame()
    
    # Find common features
    common_features = list(set(df1_available) & set(df2_available))
    
    print(f"üîç Common priority features: {len(common_features)-1 if 'Raga' in common_features else len(common_features)}")
    
    # Decide on combination strategy
    if len(common_features) >= 8:  # At least 7 features + target
        print("‚úÖ Sufficient common features - combining datasets")
        df1_subset = df1_filtered[common_features].dropna()
        df2_subset = df2_filtered[common_features].dropna()
        combined_df = pd.concat([df1_subset, df2_subset], ignore_index=True)
        
    elif len(df1_available) > len(df2_available):
        print("‚úÖ Using Dataset.csv (more features)")
        combined_df = df1_filtered.dropna()
        
    else:
        print("‚úÖ Using Final_dataset_s.csv (more features)")
        combined_df = df2_filtered.dropna()
    
    # Enhanced data cleaning
    if not combined_df.empty and 'Raga' in combined_df.columns:
        # Clean target column further
        combined_df['Raga'] = combined_df['Raga'].astype(str).str.strip()
        combined_df = combined_df[combined_df['Raga'] != '']
        combined_df = combined_df[~combined_df['Raga'].isin(['nan', 'NaN', 'None', 'null'])]
        
        # Remove ragas with insufficient samples (less than 5 for better training)
        raga_counts = combined_df['Raga'].value_counts()
        valid_ragas = raga_counts[raga_counts >= 5].index
        combined_df = combined_df[combined_df['Raga'].isin(valid_ragas)]
        
        # Handle outliers for numeric columns only
        feature_cols = [col for col in combined_df.columns if col != 'Raga']
        
        for col in feature_cols:
            if combined_df[col].dtype in ['float64', 'int64']:
                try:
                    # Convert to numeric, forcing errors to NaN
                    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')
                    
                    # Handle outliers (cap instead of remove to preserve data)
                    Q1 = combined_df[col].quantile(0.25)
                    Q3 = combined_df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # Cap outliers
                    combined_df[col] = combined_df[col].clip(lower_bound, upper_bound)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Issue with column {col}: {e}")
                    # Fill with median if conversion fails
                    combined_df[col] = combined_df[col].fillna(combined_df[col].median())
        
        # Fill any remaining missing values
        numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
        combined_df[numeric_cols] = combined_df[numeric_cols].fillna(combined_df[numeric_cols].median())
        
        print(f"\nüìä Enhanced dataset: {combined_df.shape}")
        print(f"üéº Number of ragas: {combined_df['Raga'].nunique()}")
        print(f"üéµ Ragas: {sorted(combined_df['Raga'].unique())}")
        
        # Show detailed raga distribution
        print(f"\nüìà Detailed raga distribution:")
        raga_dist = combined_df['Raga'].value_counts()
        for raga, count in raga_dist.items():
            percentage = (count / len(combined_df)) * 100
            print(f"   {raga}: {count} samples ({percentage:.1f}%)")
        
        # Feature analysis
        feature_cols = [col for col in combined_df.columns if col != 'Raga']
        print(f"\nüìä Feature analysis:")
        print(f"   Total features: {len(feature_cols)}")
        
        # Categorize features
        mfcc_count = len([f for f in feature_cols if 'mfcc' in f.lower()])
        chroma_count = len([f for f in feature_cols if 'chroma' in f.lower()])
        spectral_count = len([f for f in feature_cols if any(term in f.lower() for term in ['spectral', 'spec_'])])
        temporal_count = len([f for f in feature_cols if any(term in f.lower() for term in ['zcr', 'tempo', 'zero_crossing'])])
        other_count = len(feature_cols) - mfcc_count - chroma_count - spectral_count - temporal_count
        
        print(f"   üéµ MFCC features: {mfcc_count}")
        print(f"   üéµ Chroma features: {chroma_count}")
        print(f"   üéµ Spectral features: {spectral_count}")
        print(f"   üéµ Temporal features: {temporal_count}")
        print(f"   üéµ Other features: {other_count}")
    
    return combined_df

def create_dataset_visualizations(df):
    """Create enhanced dataset visualizations"""
    
    if df.empty:
        print("‚ùå Dataset is empty")
        return
    
    print("üé® Creating enhanced dataset visualizations...")
    
    # 1. Enhanced Raga Distribution
    plt.figure(figsize=(16, 10))
    raga_counts = df['Raga'].value_counts()
    
    # Create better color palette
    colors = sns.color_palette("husl", len(raga_counts))
    bars = plt.barh(range(len(raga_counts)), raga_counts.values, color=colors)
    
    plt.yticks(range(len(raga_counts)), raga_counts.index)
    plt.xlabel('Number of Samples', fontsize=14, fontweight='bold')
    plt.ylabel('Raga', fontsize=14, fontweight='bold')
    plt.title('Enhanced Raga Distribution in Dataset', fontsize=18, fontweight='bold', pad=20)
    
    # Add value labels and percentages
    total_samples = len(df)
    for i, (bar, count) in enumerate(zip(bars, raga_counts.values)):
        percentage = (count / total_samples) * 100
        plt.text(bar.get_width() + max(raga_counts.values)*0.01, 
                bar.get_y() + bar.get_height()/2, 
                f'{count} ({percentage:.1f}%)', ha='left', va='center', fontweight='bold')
    
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('results/graphs/enhanced_raga_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Feature Type Distribution
    feature_cols = [col for col in df.columns if col != 'Raga']
    
    if len(feature_cols) > 0:
        # Categorize features
        feature_categories = {
            'MFCC': [f for f in feature_cols if 'mfcc' in f.lower()],
            'Chroma': [f for f in feature_cols if 'chroma' in f.lower()],
            'Spectral': [f for f in feature_cols if any(term in f.lower() for term in ['spectral', 'spec_'])],
            'Temporal': [f for f in feature_cols if any(term in f.lower() for term in ['zcr', 'tempo', 'zero_crossing'])],
            'Other': [f for f in feature_cols if not any(cat_name.lower() in f.lower() 
                     for cat_name in ['mfcc', 'chroma', 'spectral', 'spec_', 'zcr', 'tempo', 'zero_crossing'])]
        }
        
        # Create pie chart
        plt.figure(figsize=(12, 8))
        sizes = [len(features) for features in feature_categories.values() if len(features) > 0]
        labels = [cat for cat, features in feature_categories.items() if len(features) > 0]
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']
        
        plt.pie(sizes, labels=labels, colors=colors[:len(sizes)], autopct='%1.1f%%', startangle=90)
        plt.title('Feature Type Distribution', fontsize=16, fontweight='bold')
        plt.axis('equal')
        plt.tight_layout()
        plt.savefig('results/graphs/feature_type_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. Dataset Statistics
    plt.figure(figsize=(14, 8))
    
    quality_metrics = {
        'Total Samples': len(df),
        'Unique Ragas': df['Raga'].nunique(),
        'Total Features': len(feature_cols),
        'Min Samples/Raga': df['Raga'].value_counts().min(),
        'Max Samples/Raga': df['Raga'].value_counts().max(),
        'Avg Samples/Raga': int(df['Raga'].value_counts().mean())
    }
    
    bars = plt.bar(range(len(quality_metrics)), list(quality_metrics.values()), 
                   color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum', 'lightsteelblue'])
    
    plt.title('Dataset Statistics', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Metrics', fontsize=14, fontweight='bold')
    plt.ylabel('Count', fontsize=14, fontweight='bold')
    plt.xticks(range(len(quality_metrics)), list(quality_metrics.keys()), rotation=45, ha='right')
    
    # Add value labels
    for bar, value in zip(bars, quality_metrics.values()):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,
                f'{value}', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig('results/graphs/dataset_statistics.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úÖ Enhanced dataset visualizations saved!")
    print("üìä Generated: enhanced_raga_distribution.png, feature_type_distribution.png, dataset_statistics.png")

print("‚úÖ Part 5: Enhanced data preparation (Fixed for concatenated raga names) ready!")
print("üîß Key fixes:")
print("   ‚Ä¢ Handles concatenated raga names like 'BhairavBhairavBhairav...'")
print("   ‚Ä¢ Extracts individual raga names using pattern matching")
print("   ‚Ä¢ Robust numeric conversion with error handling")
print("   ‚Ä¢ Better feature categorization and mapping")
print("   ‚Ä¢ Enhanced data cleaning and outlier handling")

# Part 6: Enhanced Model (Refined for Better Accuracy)

from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import Pipeline

class EnhancedRagaClassifier:
    def __init__(self):
        self.model = None
        self.feature_selector = None
        self.scaler = RobustScaler()  # More robust to outliers than StandardScaler
        self.label_encoder = LabelEncoder()
        self.feature_columns = []
        self.selected_features = []
        self.performance_metrics = {}
        
    def create_feature_selection_pipeline(self, X, y):
        """Create intelligent feature selection pipeline"""
        
        print("üîç Performing intelligent feature selection...")
        
        # Calculate optimal number of features (rule of thumb: sqrt of total features)
        n_features = X.shape[1]
        optimal_k = min(max(int(np.sqrt(n_features)), 10), n_features)
        
        print(f"   üìä Total features: {n_features}")
        print(f"   üìä Optimal features: {optimal_k}")
        
        # Use mutual information for feature selection (better for categorical targets)
        self.feature_selector = SelectKBest(
            score_func=mutual_info_classif,
            k=optimal_k
        )
        
        # Fit feature selector
        X_selected = self.feature_selector.fit_transform(X, y)
        
        # Get selected feature names
        selected_indices = self.feature_selector.get_support(indices=True)
        self.selected_features = [self.feature_columns[i] for i in selected_indices]
        
        print(f"‚úÖ Selected {len(self.selected_features)} most discriminative features")
        print(f"   üî• Top features: {self.selected_features[:5]}")
        
        return X_selected
    
    def create_optimized_ensemble(self):
        """Create highly optimized ensemble for raga classification"""
        
        # Enhanced Random Forest with optimal parameters
        rf_model = RandomForestClassifier(
            n_estimators=500,          # More trees for better performance
            max_depth=None,            # No depth limit for complex patterns
            min_samples_split=2,       # Allow fine splits
            min_samples_leaf=1,        # Capture detailed patterns
            max_features='log2',       # Log2 often works better than sqrt for classification
            bootstrap=True,
            oob_score=True,           # Out-of-bag scoring
            random_state=42,
            class_weight='balanced_subsample',  # Better handling of imbalanced data
            n_jobs=-1,
            criterion='gini'          # Gini for classification
        )
        
        # Enhanced Gradient Boosting
        gb_model = GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,       # Lower learning rate for better performance
            max_depth=6,              # Moderate depth
            min_samples_split=5,
            min_samples_leaf=3,
            subsample=0.8,
            max_features='sqrt',
            random_state=42,
            validation_fraction=0.1,
            n_iter_no_change=20      # Early stopping
        )
        
        # Enhanced SVM with probability
        svm_model = SVC(
            C=100,                    # Higher C for complex patterns
            gamma='auto',             # Auto gamma
            kernel='rbf',
            probability=True,
            random_state=42,
            class_weight='balanced',
            decision_function_shape='ovr'  # One-vs-rest for multiclass
        )
        
        # Create weighted voting ensemble
        ensemble = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('gb', gb_model),
                ('svm', svm_model)
            ],
            voting='soft',
            weights=[0.4, 0.4, 0.2],  # RF and GB get more weight
            n_jobs=-1
        )
        
        return ensemble
    
    def train(self, df):
        """Enhanced training with feature selection and optimization"""
        if df.empty:
            return {'error': 'Dataset is empty'}
        
        print("\nüöÄ Training Enhanced Raga Classifier (Optimized)...")
        print("=" * 70)
        
        # Prepare data
        X = df.drop('Raga', axis=1)
        y = df['Raga']
        
        self.feature_columns = X.columns.tolist()
        print(f"üìä Initial features: {len(self.feature_columns)}")
        
        # Handle missing values more intelligently
        X = X.fillna(X.median())  # Use median instead of 0
        
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y)
        num_classes = len(self.label_encoder.classes_)
        
        print(f"üéº Raga classes: {num_classes}")
        
        if num_classes < 2:
            return {'error': 'Need at least 2 raga classes'}
        
        # Enhanced train-test split with more data for training
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, 
            test_size=0.15,           # Smaller test set, more training data
            random_state=42, 
            stratify=y_encoded
        )
        
        print(f"üìà Training samples: {X_train.shape[0]}")
        print(f"üìà Test samples: {X_test.shape[0]}")
        
        # Feature selection on training data only
        X_train_selected = self.create_feature_selection_pipeline(X_train, y_train)
        X_test_selected = self.feature_selector.transform(X_test)
        
        print(f"üìä Features after selection: {X_train_selected.shape[1]}")
        
        # Enhanced scaling with robust scaler
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        # Create and train optimized model
        self.model = self.create_optimized_ensemble()
        
        print("ü§ñ Training optimized ensemble model...")
        self.model.fit(X_train_scaled, y_train)
        
        # Comprehensive evaluation
        print("üìä Evaluating model performance...")
        
        # Predictions
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)
        
        # Calculate metrics
        test_accuracy = accuracy_score(y_test, y_pred)
        
        # Enhanced cross-validation with more folds
        cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=10, scoring='accuracy')
        
        # Classification report
        class_names = self.label_encoder.classes_
        report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        
        # Feature importance from the best performing model
        feature_importance = {}
        try:
            rf_estimator = self.model.named_estimators_['rf']
            if hasattr(rf_estimator, 'feature_importances_'):
                feature_importance = dict(zip(self.selected_features, rf_estimator.feature_importances_))
                print(f"‚úÖ Feature importance extracted from Random Forest")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not extract feature importance: {e}")
        
        # Store enhanced performance metrics
        self.performance_metrics = {
            'test_accuracy': float(test_accuracy),
            'cv_mean': float(np.mean(cv_scores)),
            'cv_std': float(np.std(cv_scores)),
            'cv_scores': [float(x) for x in cv_scores.tolist()],
            'classification_report': report,
            'confusion_matrix': [[int(x) for x in row] for row in cm.tolist()],
            'class_names': [str(x) for x in class_names.tolist()],
            'feature_importance': {k: float(v) for k, v in feature_importance.items()},
            'selected_features': [str(x) for x in self.selected_features],
            'num_features_original': int(len(self.feature_columns)),
            'num_features_selected': int(len(self.selected_features)),
            'num_classes': int(num_classes),
            'training_samples': int(X_train.shape[0]),
            'test_samples': int(X_test.shape[0])
        }
        
        # Enhanced result reporting
        print(f"\n‚úÖ Enhanced training completed!")
        print(f"üéØ Test Accuracy: {test_accuracy:.4f}")
        print(f"üéØ CV Accuracy: {np.mean(cv_scores):.4f} (¬±{np.std(cv_scores):.4f})")
        print(f"üìä Macro Avg F1: {report.get('macro avg', {}).get('f1-score', 0):.4f}")
        print(f"üìä Weighted Avg F1: {report.get('weighted avg', {}).get('f1-score', 0):.4f}")
        print(f"üîç Features: {len(self.feature_columns)} ‚Üí {len(self.selected_features)} selected")
        
        # Show per-class performance for insight
        print(f"\nüìä Per-class accuracy (top 5):")
        class_accuracies = []
        for i, class_name in enumerate(class_names):
            if class_name in report and isinstance(report[class_name], dict):
                f1_score = report[class_name].get('f1-score', 0)
                class_accuracies.append((class_name, f1_score))
        
        # Sort by F1 score and show top 5
        class_accuracies.sort(key=lambda x: x[1], reverse=True)
        for class_name, f1_score in class_accuracies[:5]:
            print(f"   {class_name}: F1={f1_score:.4f}")
        
        return self.performance_metrics
    
    def predict_audio(self, audio_path):
        """Enhanced prediction with better feature alignment"""
        if self.model is None:
            return {'error': 'Model not trained yet'}
        
        print(f"\nüéµ Enhanced analysis: {os.path.basename(audio_path)}")
        
        # Extract features
        features = extract_all_features_with_viz(audio_path)
        
        if not features:
            return {'error': 'Failed to extract features'}
        
        # Align features with training features using intelligent mapping
        aligned_features = self.align_features_intelligently(features)
        
        # Prepare feature vector for selected features only
        feature_vector = []
        missing_features = []
        
        for col in self.selected_features:
            if col in aligned_features:
                value = aligned_features[col]
                if np.isnan(value) or np.isinf(value):
                    # Use feature median instead of 0
                    feature_vector.append(0)  # Could be improved with stored medians
                    missing_features.append(col)
                else:
                    feature_vector.append(value)
            else:
                feature_vector.append(0)
                missing_features.append(col)
        
        print(f"üìä Features extracted: {len(features)}")
        print(f"üìä Features aligned: {len(aligned_features)}")
        print(f"üìä Selected features used: {len(self.selected_features)}")
        if missing_features:
            print(f"‚ö†Ô∏è Missing features: {len(missing_features)}")
        
        # Scale and predict using the same pipeline
        feature_vector = np.array(feature_vector).reshape(1, -1)
        feature_vector_scaled = self.scaler.transform(feature_vector)
        
        # Get predictions from ensemble
        prediction = self.model.predict(feature_vector_scaled)[0]
        probabilities = self.model.predict_proba(feature_vector_scaled)[0]
        
        # Get raga name and confidence
        predicted_raga = self.label_encoder.inverse_transform([prediction])[0]
        confidence = float(np.max(probabilities))
        
        # All class probabilities
        all_probabilities = {
            raga: float(prob) 
            for raga, prob in zip(self.label_encoder.classes_, probabilities)
        }
        
        # Top predictions
        sorted_predictions = sorted(all_probabilities.items(), key=lambda x: x[1], reverse=True)
        
        # Individual model predictions for transparency
        individual_predictions = {}
        try:
            for name, estimator in self.model.named_estimators_.items():
                pred = estimator.predict(feature_vector_scaled)[0]
                pred_raga = self.label_encoder.inverse_transform([pred])[0]
                pred_proba = estimator.predict_proba(feature_vector_scaled)[0]
                individual_predictions[name] = {
                    'predicted_raga': pred_raga,
                    'confidence': float(np.max(pred_proba))
                }
        except:
            pass
        
        # Enhanced quality metrics
        feature_quality_score = (len(aligned_features) - len(missing_features)) / len(self.selected_features)
        alignment_score = len(aligned_features) / len(features) if features else 0
        
        result = {
            'predicted_raga': predicted_raga,
            'confidence': confidence,
            'all_probabilities': all_probabilities,
            'top_5_predictions': sorted_predictions[:5],
            'individual_models': individual_predictions,
            'features_extracted': len(features),
            'features_aligned': len(aligned_features),
            'features_used': len(self.selected_features),
            'missing_features_count': len(missing_features),
            'feature_quality_score': feature_quality_score,
            'alignment_score': alignment_score
        }
        
        print(f"‚úÖ Prediction: {predicted_raga}")
        print(f"üéØ Confidence: {confidence:.4f}")
        print(f"üìä Feature Quality: {feature_quality_score:.2f}")
        print(f"üìä Alignment Score: {alignment_score:.2f}")
        
        return result
    
    def align_features_intelligently(self, extracted_features):
        """Intelligently align extracted features with training features"""
        
        aligned = {}
        
        # Feature mapping for intelligent alignment
        feature_mappings = {
            # MFCC mappings
            'mfcc_0': ['mfcc0', 'mfcc_0', 'speechpy_mfcc_0_mean', 'parselmouth_mfcc_0'],
            'mfcc_1': ['mfcc1', 'mfcc_1', 'speechpy_mfcc_1_mean', 'parselmouth_mfcc_1'],
            'mfcc_2': ['mfcc2', 'mfcc_2', 'speechpy_mfcc_2_mean', 'parselmouth_mfcc_2'],
            'mfcc_3': ['mfcc3', 'mfcc_3', 'speechpy_mfcc_3_mean', 'parselmouth_mfcc_3'],
            'mfcc_4': ['mfcc4', 'mfcc_4', 'speechpy_mfcc_4_mean', 'parselmouth_mfcc_4'],
            'mfcc_5': ['mfcc5', 'mfcc_5', 'speechpy_mfcc_5_mean', 'parselmouth_mfcc_5'],
            'mfcc_6': ['mfcc6', 'mfcc_6', 'speechpy_mfcc_6_mean', 'parselmouth_mfcc_6'],
            'mfcc_7': ['mfcc7', 'mfcc_7', 'speechpy_mfcc_7_mean', 'parselmouth_mfcc_7'],
            'mfcc_8': ['mfcc8', 'mfcc_8', 'speechpy_mfcc_8_mean', 'parselmouth_mfcc_8'],
            'mfcc_9': ['mfcc9', 'mfcc_9', 'speechpy_mfcc_9_mean', 'parselmouth_mfcc_9'],
            'mfcc_10': ['mfcc10', 'mfcc_10', 'speechpy_mfcc_10_mean', 'parselmouth_mfcc_10'],
            'mfcc_11': ['mfcc11', 'mfcc_11', 'speechpy_mfcc_11_mean', 'parselmouth_mfcc_11'],
            'mfcc_12': ['mfcc12', 'mfcc_12', 'speechpy_mfcc_12_mean', 'parselmouth_mfcc_12'],
            
            # Chroma mappings
            'chroma_1': ['chroma1', 'chroma_1_mean', 'chroma_1', 'chroma_stft'],
            'chroma_2': ['chroma2', 'chroma_2_mean', 'chroma_2'],
            'chroma_3': ['chroma3', 'chroma_3_mean', 'chroma_3'],
            'chroma_4': ['chroma4', 'chroma_4_mean', 'chroma_4'],
            'chroma_5': ['chroma5', 'chroma_5_mean', 'chroma_5'],
            'chroma_6': ['chroma6', 'chroma_6_mean', 'chroma_6'],
            'chroma_7': ['chroma7', 'chroma_7_mean', 'chroma_7'],
            'chroma_8': ['chroma8', 'chroma_8_mean', 'chroma_8'],
            'chroma_9': ['chroma9', 'chroma_9_mean', 'chroma_9'],
            'chroma_10': ['chroma10', 'chroma_10_mean', 'chroma_10'],
            'chroma_11': ['chroma11', 'chroma_11_mean', 'chroma_11'],
            'chroma_12': ['chroma12', 'chroma_12_mean', 'chroma_12'],
            
            # Spectral mappings
            'spectral_centroid': ['spectral_centroid', 'spec_cent', 'speechpy_spectral_centroid'],
            'spectral_bandwidth': ['spectral_bandwidth', 'spec_bw', 'speechpy_spectral_bandwidth'],
            
            # Temporal mappings
            'zcr_mean': ['zcr_mean', 'zero_crossing_rate', 'zcr'],
            'tempo_estimated': ['tempo_estimated', 'tempo', 'tempo_bpm']
        }
        
        # Map each training feature to extracted features
        for training_feature in self.feature_columns:
            mapped_value = None
            
            # Direct match first
            if training_feature in extracted_features:
                mapped_value = extracted_features[training_feature]
            else:
                # Try feature mappings
                if training_feature in feature_mappings:
                    for candidate in feature_mappings[training_feature]:
                        if candidate in extracted_features:
                            mapped_value = extracted_features[candidate]
                            break
                
                # If still not found, try partial matching
                if mapped_value is None:
                    for ext_feature, value in extracted_features.items():
                        if training_feature.lower() in ext_feature.lower() or ext_feature.lower() in training_feature.lower():
                            mapped_value = value
                            break
            
            if mapped_value is not None:
                aligned[training_feature] = mapped_value
        
        return aligned

# Don't create model visualization functions - only feature visualizations as requested

print("‚úÖ Part 6: Enhanced model (refined for better accuracy) ready!")
print("üî• Key improvements:")
print("   ‚Ä¢ Intelligent feature selection using mutual information")
print("   ‚Ä¢ Robust scaling for better outlier handling")
print("   ‚Ä¢ Optimized ensemble with weighted voting")
print("   ‚Ä¢ Smart feature alignment for prediction")
print("   ‚Ä¢ Enhanced cross-validation (10-fold)")
print("   ‚Ä¢ Better missing value handling")

# Part 7: Main Execution and Interface (Fixed for SpeechPy)

def predict_raga_from_file(file_path, classifier):
    """Predict raga from audio file"""
    from pathlib import Path
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        return {'error': f'File not found: {file_path}'}
    
    # Check file extension
    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac', '.au']
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
    
    ext = file_path.suffix.lower()
    
    if ext not in audio_extensions + video_extensions:
        return {'error': f'Unsupported format: {ext}. Supported: {audio_extensions + video_extensions}'}
    
    try:
        result = classifier.predict_audio(str(file_path))
        
        if 'error' not in result:
            # Add file information
            result['file_info'] = {
                'filename': file_path.name,
                'file_type': 'video' if ext in video_extensions else 'audio',
                'extension': ext,
                'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2)
            }
        
        return result
        
    except Exception as e:
        return {'error': f'Processing failed: {str(e)}'}

def interactive_mode(classifier):
    """Interactive prediction mode"""
    print("\n" + "="*70)
    print("üéº INTERACTIVE RAGA PREDICTION MODE")
    print("="*70)
    print("Enter audio/video file paths, or 'quit' to exit")
    print("Supported: .wav, .mp3, .flac, .ogg, .m4a, .aac, .mp4, .avi, .mov")
    print("-" * 70)
    
    while True:
        try:
            file_path = input("\nüìÅ Enter file path: ").strip().strip('"\'')
            
            if file_path.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break
            
            if not file_path:
                continue
            
            result = predict_raga_from_file(file_path, classifier)
            
            if 'error' not in result:
                print(f"\n‚úÖ PREDICTION RESULTS:")
                print(f"   üéµ Predicted Raga: {result['predicted_raga']}")
                print(f"   üéØ Confidence: {result['confidence']:.4f}")
                print(f"   üìÅ File: {result['file_info']['filename']}")
                print(f"   üìä Features: {result['features_extracted']}/{result['features_used']}")
                print(f"   üìä Feature Quality: {result['feature_quality_score']:.2f}")
                
                print(f"\n   üìà Top 5 Predictions:")
                for i, (raga, prob) in enumerate(result['top_5_predictions'], 1):
                    print(f"      {i}. {raga}: {prob:.4f}")
                
                # Show individual model predictions
                if result['individual_models']:
                    print(f"\n   ü§ñ Individual Model Predictions:")
                    for model_name, pred in result['individual_models'].items():
                        print(f"      {model_name}: {pred['predicted_raga']} ({pred['confidence']:.4f})")
                
                # Save result
                import datetime
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                result_file = f'results/json_values/prediction_{timestamp}.json'
                
                with open(result_file, 'w') as f:
                    json.dump(result, f, indent=2)
                
                print(f"   üíæ Result saved: {result_file}")
                
            else:
                print(f"\n‚ùå ERROR: {result['error']}")
                
        except KeyboardInterrupt:
            print("\n\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"\n‚ùå Unexpected error: {e}")

# Part 7: Refined Main Execution (Updated for Better Performance)

def main():
    """Enhanced main function with better pipeline"""
    
    print("üéº ENHANCED RAGA CLASSIFICATION SYSTEM (REFINED)")
    print("Using Parselmouth + SpeechPy with Intelligent Feature Selection")
    print("="*80)
    
    # Check requirements
    if not PARSELMOUTH_AVAILABLE:
        print("‚ùå Parselmouth required: pip install praat-parselmouth")
        return None, None
    
    if not SPEECHPY_AVAILABLE and not LIBROSA_AVAILABLE:
        print("‚ùå SpeechPy or librosa required:")
        print("   pip install speechpy")
        print("   or pip install librosa")
        return None, None
    
    # Step 1: Load datasets
    print("\nüìä Step 1: Loading and analyzing datasets...")
    df1, df2 = load_datasets()
    
    if df1 is None or df2 is None:
        print("‚ùå Failed to load datasets")
        return None, None
    
    # Step 2: Enhanced dataset preparation
    print("\nüîß Step 2: Enhanced dataset preparation...")
    combined_df = prepare_enhanced_dataset(df1, df2)
    
    if combined_df.empty:
        print("‚ùå No usable data after preparation")
        return None, None
    
    # Save enhanced dataset info
    dataset_info = {
        'total_samples': len(combined_df),
        'num_ragas': combined_df['Raga'].nunique(),
        'raga_list': sorted(combined_df['Raga'].unique()),
        'raga_distribution': combined_df['Raga'].value_counts().to_dict(),
        'feature_columns': [col for col in combined_df.columns if col != 'Raga'],
        'num_features': len([col for col in combined_df.columns if col != 'Raga']),
        'min_samples_per_raga': combined_df['Raga'].value_counts().min(),
        'max_samples_per_raga': combined_df['Raga'].value_counts().max(),
        'avg_samples_per_raga': combined_df['Raga'].value_counts().mean(),
        'dataset_balance_ratio': combined_df['Raga'].value_counts().min() / combined_df['Raga'].value_counts().max()
    }
    
    try:
        # Convert any numpy types to Python native types for JSON serialization
        performance_json = {}
        for key, value in performance.items():
            if isinstance(value, np.ndarray):
                performance_json[key] = value.tolist()
            elif isinstance(value, (np.integer, np.int64, np.int32)):
                performance_json[key] = int(value)
            elif isinstance(value, (np.floating, np.float64, np.float32)):
                performance_json[key] = float(value)
            elif isinstance(value, dict):
                # Handle nested dictionaries
                performance_json[key] = {}
                for k, v in value.items():
                    if isinstance(v, (np.integer, np.int64, np.int32)):
                        performance_json[key][k] = int(v)
                    elif isinstance(v, (np.floating, np.float64, np.float32)):
                        performance_json[key][k] = float(v)
                    else:
                        performance_json[key][k] = v
            elif isinstance(value, list):
                # Handle lists that might contain numpy types
                performance_json[key] = []
                for item in value:
                    if isinstance(item, (np.integer, np.int64, np.int32)):
                        performance_json[key].append(int(item))
                    elif isinstance(item, (np.floating, np.float64, np.float32)):
                        performance_json[key].append(float(item))
                    elif isinstance(item, list):
                        # Handle nested lists (like confusion matrix)
                        performance_json[key].append([int(x) if isinstance(x, (np.integer, np.int64, np.int32)) 
                                                    else float(x) if isinstance(x, (np.floating, np.float64, np.float32))
                                                    else x for x in item])
                    else:
                        performance_json[key].append(item)
            else:
                performance_json[key] = value
        
        with open('results/json_values/enhanced_model_performance.json', 'w') as f:
            json.dump(performance_json, f, indent=2)
        
        print("‚úÖ Performance metrics saved successfully")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not save performance JSON: {e}")
    
    # Step 3: Create enhanced dataset visualizations
    print("\nüé® Step 3: Creating enhanced dataset visualizations...")
    create_dataset_visualizations(combined_df)
    
    # Step 4: Train enhanced model
    print("\nü§ñ Step 4: Training enhanced model...")
    classifier = EnhancedRagaClassifier()
    performance = classifier.train(combined_df)
    
    if 'error' in performance:
        print(f"‚ùå Training failed: {performance['error']}")
        return None, None
    
    # Step 5: Save enhanced performance metrics
    with open('results/json_values/enhanced_model_performance.json', 'w') as f:
        json.dump(performance, f, indent=2)
    
    # Step 6: Enhanced system ready report
    print("\n" + "="*80)
    print("üéâ ENHANCED SYSTEM READY!")
    print("="*80)
    print(f"‚úÖ Model trained with {performance['test_accuracy']:.4f} accuracy")
    print(f"üéØ Cross-validation: {performance['cv_mean']:.4f} (¬±{performance['cv_std']:.4f})")
    print(f"üéº Can classify {performance['num_classes']} ragas")
    print(f"üìä Features: {performance['num_features_original']} ‚Üí {performance['num_features_selected']} selected")
    print(f"üéµ Ragas: {', '.join(performance['class_names'][:5])}...")
    if len(performance['class_names']) > 5:
        print(f"       ...and {len(performance['class_names']) - 5} more")
    
    # Show feature selection results
    print(f"\nüîç FEATURE SELECTION RESULTS:")
    print(f"   üìä Original features: {performance['num_features_original']}")
    print(f"   üìä Selected features: {performance['num_features_selected']}")
    print(f"   üìä Reduction ratio: {(1 - performance['num_features_selected']/performance['num_features_original']):.2%}")
    
    if performance['feature_importance']:
        print(f"   üî• Top 5 selected features:")
        sorted_features = sorted(performance['feature_importance'].items(), 
                               key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:5], 1):
            print(f"      {i}. {feature}: {importance:.4f}")
    
    # Enhanced model performance breakdown
    print(f"\nüìä ENHANCED PERFORMANCE BREAKDOWN:")
    report = performance['classification_report']
    print(f"   üéØ Test Accuracy: {performance['test_accuracy']:.4f}")
    print(f"   üéØ CV Accuracy: {performance['cv_mean']:.4f} (¬±{performance['cv_std']:.4f})")
    print(f"   üìä Macro Avg Precision: {report.get('macro avg', {}).get('precision', 0):.4f}")
    print(f"   üìä Macro Avg Recall: {report.get('macro avg', {}).get('recall', 0):.4f}")
    print(f"   üìä Macro Avg F1: {report.get('macro avg', {}).get('f1-score', 0):.4f}")
    print(f"   üìä Weighted Avg F1: {report.get('weighted avg', {}).get('f1-score', 0):.4f}")
    
    # Dataset quality report
    print(f"\nüìà DATASET QUALITY REPORT:")
    print(f"   üìä Total samples: {dataset_info['total_samples']}")
    print(f"   üìä Unique ragas: {dataset_info['num_ragas']}")
    print(f"   üìä Balance ratio: {dataset_info['dataset_balance_ratio']:.3f}")
    print(f"   üìä Min samples/raga: {dataset_info['min_samples_per_raga']}")
    print(f"   üìä Max samples/raga: {dataset_info['max_samples_per_raga']}")
    
    print(f"\nüìÅ ENHANCED OUTPUT FILES:")
    print(f"   üìä Dataset: results/graphs/enhanced_raga_distribution.png")
    print(f"   üìä Features: results/graphs/feature_distributions.png")
    print(f"   üìä Quality: results/graphs/dataset_quality_metrics.png")
    print(f"   üíæ Dataset info: results/json_values/enhanced_dataset_info.json")
    print(f"   üíæ Model performance: results/json_values/enhanced_model_performance.json")
    print(f"   üé® Feature analysis: results/graphs/visualizations/ (when processing audio)")
    
    return classifier, performance

def enhanced_predict_raga_from_file(file_path, classifier):
    """Enhanced prediction with better error handling and reporting"""
    from pathlib import Path
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        return {'error': f'File not found: {file_path}'}
    
    # Check file extension
    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac', '.au']
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
    
    ext = file_path.suffix.lower()
    
    if ext not in audio_extensions + video_extensions:
        return {'error': f'Unsupported format: {ext}. Supported: {audio_extensions + video_extensions}'}
    
    try:
        result = classifier.predict_audio(str(file_path))
        
        if 'error' not in result:
            # Add enhanced file information
            result['file_info'] = {
                'filename': file_path.name,
                'file_type': 'video' if ext in video_extensions else 'audio',
                'extension': ext,
                'file_size_mb': round(file_path.stat().st_size / (1024*1024), 2),
                'processing_timestamp': pd.Timestamp.now().isoformat()
            }
            
            # Add quality assessment
            confidence = result['confidence']
            if confidence >= 0.7:
                quality_assessment = "High confidence prediction"
            elif confidence >= 0.5:
                quality_assessment = "Moderate confidence prediction"
            elif confidence >= 0.3:
                quality_assessment = "Low confidence prediction"
            else:
                quality_assessment = "Very low confidence - consider audio quality"
            
            result['quality_assessment'] = quality_assessment
        
        return result
        
    except Exception as e:
        return {'error': f'Processing failed: {str(e)}'}

def enhanced_interactive_mode(classifier):
    """Enhanced interactive mode with better feedback"""
    print("\n" + "="*80)
    print("üéº ENHANCED INTERACTIVE RAGA PREDICTION MODE")
    print("="*80)
    print("Enter audio/video file paths, or 'quit' to exit")
    print("Supported: .wav, .mp3, .flac, .ogg, .m4a, .aac, .mp4, .avi, .mov")
    print("Features: Enhanced accuracy, intelligent feature selection, quality assessment")
    print("-" * 80)
    
    prediction_count = 0
    
    while True:
        try:
            file_path = input(f"\nüìÅ Enter file path (prediction #{prediction_count + 1}): ").strip().strip('"\'')
            
            if file_path.lower() in ['quit', 'exit', 'q']:
                print(f"üëã Session complete! Made {prediction_count} predictions.")
                break
            
            if not file_path:
                continue
            
            print(f"\nüîÑ Processing file #{prediction_count + 1}...")
            result = enhanced_predict_raga_from_file(file_path, classifier)
            
            if 'error' not in result:
                prediction_count += 1
                
                print(f"\n‚úÖ ENHANCED PREDICTION RESULTS:")
                print(f"   üéµ Predicted Raga: {result['predicted_raga']}")
                print(f"   üéØ Confidence: {result['confidence']:.4f}")
                print(f"   üìä Quality Assessment: {result['quality_assessment']}")
                print(f"   üìÅ File: {result['file_info']['filename']}")
                print(f"   üìä Feature Alignment: {result['alignment_score']:.2f}")
                print(f"   üìä Feature Quality: {result['feature_quality_score']:.2f}")
                
                print(f"\n   üìà Top 5 Predictions:")
                for i, (raga, prob) in enumerate(result['top_5_predictions'], 1):
                    conf_indicator = "üî•" if prob > 0.5 else "‚ö°" if prob > 0.3 else "üí´"
                    print(f"      {i}. {conf_indicator} {raga}: {prob:.4f}")
                
                # Enhanced model breakdown
                if result['individual_models']:
                    print(f"\n   ü§ñ Enhanced Model Breakdown:")
                    model_names = {'rf': 'Random Forest', 'gb': 'Gradient Boosting', 'svm': 'Support Vector Machine'}
                    for model_code, pred in result['individual_models'].items():
                        model_name = model_names.get(model_code, model_code)
                        agreement = "‚úÖ" if pred['predicted_raga'] == result['predicted_raga'] else "‚ùå"
                        print(f"      {agreement} {model_name}: {pred['predicted_raga']} ({pred['confidence']:.4f})")
                
                # Enhanced technical details
                print(f"\n   üîß Technical Details:")
                print(f"      ‚Ä¢ Features extracted: {result['features_extracted']}")
                print(f"      ‚Ä¢ Features aligned: {result['features_aligned']}")
                print(f"      ‚Ä¢ Selected features used: {result['features_used']}")
                print(f"      ‚Ä¢ Missing features: {result['missing_features_count']}")
                
                # Save enhanced result
                timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
                result_file = f'results/json_values/enhanced_prediction_{timestamp}.json'
                
                with open(result_file, 'w') as f:
                    json.dump(result, f, indent=2)
                
                print(f"   üíæ Enhanced result saved: {result_file}")
                
            else:
                print(f"\n‚ùå ERROR: {result['error']}")
                print(f"üí° Tips: Check file format, file path, and audio quality")
                
        except KeyboardInterrupt:
            print(f"\n\nüëã Session interrupted! Made {prediction_count} predictions.")
            break
        except Exception as e:
            print(f"\n‚ùå Unexpected error: {e}")

# Quick utility functions
def enhanced_quick_predict(file_path, classifier=None):
    """Enhanced quick prediction function"""
    if classifier is None:
        print("‚ùå Run main() first to train the enhanced model")
        return None
    return enhanced_predict_raga_from_file(file_path, classifier)

def show_enhanced_model_info(performance=None):
    """Show enhanced model information"""
    if performance is None:
        print("‚ùå No model performance data available")
        return
    
    print(f"\nüìä ENHANCED MODEL PERFORMANCE SUMMARY")
    print("="*60)
    print(f"üéØ Test Accuracy: {performance['test_accuracy']:.4f}")
    print(f"üéØ CV Accuracy: {performance['cv_mean']:.4f} (¬±{performance['cv_std']:.4f})")
    print(f"üìà Training Samples: {performance['training_samples']}")
    print(f"üìà Test Samples: {performance['test_samples']}")
    print(f"üéº Raga Classes: {performance['num_classes']}")
    print(f"üìä Features: {performance['num_features_original']} ‚Üí {performance['num_features_selected']}")
    
    report = performance['classification_report']
    print(f"üìä Macro Avg Precision: {report.get('macro avg', {}).get('precision', 0):.4f}")
    print(f"üìä Macro Avg Recall: {report.get('macro avg', {}).get('recall', 0):.4f}")
    print(f"üìä Macro Avg F1-Score: {report.get('macro avg', {}).get('f1-score', 0):.4f}")
    
    print(f"\nüìä SELECTED FEATURES:")
    if 'selected_features' in performance:
        features = performance['selected_features']
        for i in range(0, len(features), 4):
            feature_group = features[i:i+4]
            print(f"   {' | '.join(feature_group)}")
    
    print(f"\nüéµ FEATURE LIBRARIES:")
    print(f"   üéµ Parselmouth: MFCCs, Spectral Centroid, Spectral Bandwidth, Pitch, Formants")
    print(f"   üéµ SpeechPy: Chroma, Tempo, Zero Crossing Rate, Additional Features")
    print("="*60)

print("‚úÖ Part 7: Enhanced main execution (refined for better performance) ready!")
print("\nüöÄ ENHANCED USAGE:")
print("1. Run: classifier, performance = main()")
print("2. Then: enhanced_interactive_mode(classifier)")
print("3. Or: enhanced_quick_predict('your_file.mp3', classifier)")
print("4. Check: show_enhanced_model_info(performance)")

# Part 8: Complete Execution Script (Fixed and Updated)

if __name__ == "__main__":
    print("üéº STARTING COMPLETE RAGA CLASSIFICATION PIPELINE")
    print("="*80)
    
    # Check if data files exist
    if not os.path.exists('data/Dataset.csv'):
        print("‚ùå Missing data/Dataset.csv")
        print("Please ensure your data files are in the 'data/' directory")
        exit(1)
    
    if not os.path.exists('data/Final_dataset_s.csv'):
        print("‚ùå Missing data/Final_dataset_s.csv")
        print("Please ensure your data files are in the 'data/' directory")
        exit(1)
    
    # Check required libraries
    if not PARSELMOUTH_AVAILABLE:
        print("‚ùå Missing Parselmouth: pip install praat-parselmouth")
        exit(1)
    
    if not SPEECHPY_AVAILABLE and not LIBROSA_AVAILABLE:
        print("‚ùå Missing audio libraries:")
        print("   pip install speechpy")
        print("   or pip install librosa")
        exit(1)
    
    # Run main pipeline
    try:
        classifier, performance = main()
        
        if classifier is not None and performance is not None:
            print("\nüéä SUCCESS! System is ready for predictions!")
            
            # Show detailed results using correct function name
            show_enhanced_model_info(performance)
            
            print(f"\nüé® VISUALIZATIONS CREATED:")
            print(f"üìä Dataset Analysis:")
            print(f"   ‚Ä¢ enhanced_raga_distribution.png")
            print(f"   ‚Ä¢ feature_distributions.png") 
            print(f"   ‚Ä¢ dataset_quality_metrics.png")
            print(f"üé® Feature Visualizations (when processing audio):")
            print(f"   ‚Ä¢ results/graphs/visualizations/[filename]_feature_analysis.png")
            print(f"   ‚Ä¢ results/graphs/visualizations/[filename]_zcr_histogram.png")
            print(f"   ‚Ä¢ results/graphs/visualizations/[filename]_chroma_analysis.png")
            print(f"   ‚Ä¢ results/graphs/visualizations/[filename]_mfcc_analysis.png")
            print(f"   ‚Ä¢ results/graphs/visualizations/[filename]_feature_comparison.png")
            
            print(f"\nüíæ JSON DATA SAVED:")
            print(f"   ‚Ä¢ results/json_values/enhanced_dataset_info.json")
            print(f"   ‚Ä¢ results/json_values/enhanced_model_performance.json")
            print(f"   ‚Ä¢ results/json_values/enhanced_prediction_[timestamp].json (for each prediction)")
            
            print(f"\nüöÄ READY FOR PREDICTIONS!")
            print(f"Choose an option:")
            print(f"1. Interactive mode - Enter file paths manually")
            print(f"2. Quick prediction - Provide a file path now") 
            print(f"3. Exit")
            
            choice = input("\nEnter your choice (1-3): ").strip()
            
            if choice == '1':
                print(f"\nüéÆ Starting enhanced interactive mode...")
                enhanced_interactive_mode(classifier)
            elif choice == '2':
                file_path = input("Enter audio/video file path: ").strip().strip('"\'')
                if file_path:
                    print(f"\nüéµ Processing single file...")
                    result = enhanced_quick_predict(file_path, classifier)
                    if result and 'error' not in result:
                        print(f"\n‚úÖ PREDICTION COMPLETE!")
                        print(f"üéµ Predicted Raga: {result['predicted_raga']}")
                        print(f"üéØ Confidence: {result['confidence']:.4f}")
                        print(f"üìä Quality Assessment: {result['quality_assessment']}")
                        print(f"üìä Feature Quality: {result['feature_quality_score']:.2f}")
                        print(f"üìä Alignment Score: {result['alignment_score']:.2f}")
                        
                        print(f"\nüìà Top 3 Predictions:")
                        for i, (raga, prob) in enumerate(result['top_5_predictions'][:3], 1):
                            conf_indicator = "üî•" if prob > 0.5 else "‚ö°" if prob > 0.3 else "üí´"
                            print(f"   {i}. {conf_indicator} {raga}: {prob:.4f}")
                        
                        # Show individual model predictions
                        if result.get('individual_models'):
                            print(f"\nü§ñ Enhanced Model Breakdown:")
                            model_display = {'rf': 'Random Forest', 'gb': 'Gradient Boosting', 'svm': 'Support Vector Machine'}
                            for model_name, pred in result['individual_models'].items():
                                display_name = model_display.get(model_name, model_name)
                                agreement = "‚úÖ" if pred['predicted_raga'] == result['predicted_raga'] else "‚ùå"
                                print(f"   {agreement} {display_name}: {pred['predicted_raga']} ({pred['confidence']:.4f})")
                        
                        print(f"\nüìä Enhanced Feature Extraction Summary:")
                        print(f"   ‚Ä¢ Total features extracted: {result['features_extracted']}")
                        print(f"   ‚Ä¢ Features aligned: {result['features_aligned']}")
                        print(f"   ‚Ä¢ Selected features used: {result['features_used']}")
                        print(f"   ‚Ä¢ Missing features: {result['missing_features_count']}")
                        print(f"   ‚Ä¢ Feature quality score: {result['feature_quality_score']:.2%}")
                        print(f"   ‚Ä¢ Alignment score: {result['alignment_score']:.2%}")
                        
                        # Save enhanced result with timestamp
                        import datetime
                        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                        result_file = f'results/json_values/enhanced_prediction_{timestamp}.json'
                        
                        with open(result_file, 'w') as f:
                            json.dump(result, f, indent=2)
                        
                        print(f"   üíæ Enhanced result saved: {result_file}")
                        
                    else:
                        print(f"‚ùå Prediction failed: {result.get('error', 'Unknown error')}")
                        print(f"üí° Tips: Check file format, file path, and audio quality")
            elif choice == '3':
                print(f"üëã Goodbye!")
            else:
                print(f"Invalid choice. Exiting...")
                
        else:
            print("‚ùå System initialization failed")
            
    except KeyboardInterrupt:
        print(f"\n\nüëã Process interrupted by user. Goodbye!")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        print(f"Please check your data files and library installations.")

# Additional utility functions for external use (Fixed)
def load_trained_model():
    """Load a pre-trained model (if you want to save/load models)"""
    print("‚ÑπÔ∏è Model saving/loading not implemented in this version.")
    print("Run main() to train a new model.")

def test_system():
    """Test system with enhanced components"""
    print("üß™ Testing enhanced system components...")
    
    # Test imports
    print(f"‚úÖ Parselmouth: {'Available' if PARSELMOUTH_AVAILABLE else 'Not Available'}")
    print(f"‚úÖ SpeechPy: {'Available' if SPEECHPY_AVAILABLE else 'Not Available'}")
    print(f"‚úÖ librosa: {'Available' if LIBROSA_AVAILABLE else 'Not Available'}")
    print(f"‚úÖ scipy: {'Available' if SCIPY_AVAILABLE else 'Not Available'}")
    
    # Test data loading
    try:
        df1, df2 = load_datasets()
        if df1 is not None and df2 is not None:
            print("‚úÖ Dataset loading: Success")
            print(f"   Dataset.csv: {df1.shape}")
            print(f"   Final_dataset_s.csv: {df2.shape}")
            
            # Test enhanced preparation
            combined_df = prepare_enhanced_dataset(df1, df2)
            if not combined_df.empty:
                print("‚úÖ Enhanced dataset preparation: Success")
                print(f"   Enhanced dataset: {combined_df.shape}")
                print(f"   Unique ragas: {combined_df['Raga'].nunique()}")
            else:
                print("‚ùå Enhanced dataset preparation: Failed")
        else:
            print("‚ùå Dataset loading: Failed")
    except Exception as e:
        print(f"‚ùå Dataset loading: Failed ({e})")
    
    print("üß™ Enhanced system test complete!")

def show_feature_libraries():
    """Show which feature extraction libraries are being used"""
    print("\nüéµ ENHANCED FEATURE EXTRACTION LIBRARIES")
    print("="*60)
    print("üìä PARSELMOUTH (Praat-based):")
    print("   ‚úÖ MFCCs (13 coefficients + statistics)")
    print("   ‚úÖ Spectral Centroid (time series)")
    print("   ‚úÖ Spectral Bandwidth (time series)")
    print("   ‚úÖ Pitch features (mean, std, range, median)")
    print("   ‚úÖ Formant analysis (F1, F2, F3)")
    print()
    print("üìä SPEECHPY (Enhanced):")
    print("   ‚úÖ Chroma features (12 pitch classes + statistics)")
    print("   ‚úÖ Zero Crossing Rate (frame-based + statistics)")
    print("   ‚úÖ Tempo estimation (energy-based)")
    print("   ‚úÖ Additional MFCCs (backup)")
    print("   ‚úÖ Enhanced spectral features")
    print()
    print("üìä FALLBACK (Librosa):")
    print("   üîÑ Used when SpeechPy fails")
    print("   üîÑ Provides basic feature extraction")
    print("   üîÑ Tempo and beat tracking")
    print()
    print("üß† INTELLIGENT FEATURES:")
    print("   üî• Feature selection using mutual information")
    print("   üî• Robust scaling for outlier handling")
    print("   üî• Smart feature alignment")
    print("   üî• Quality assessment metrics")
    print("="*60)

def show_system_capabilities():
    """Show enhanced system capabilities"""
    print("\nüöÄ ENHANCED SYSTEM CAPABILITIES")
    print("="*60)
    print("üéµ AUDIO PROCESSING:")
    print("   ‚Ä¢ Supports: .wav, .mp3, .flac, .ogg, .m4a, .aac, .au")
    print("   ‚Ä¢ Video audio extraction: .mp4, .avi, .mov, .mkv, .wmv")
    print("   ‚Ä¢ Automatic format detection")
    print("   ‚Ä¢ Quality assessment")
    print()
    print("üß† MACHINE LEARNING:")
    print("   ‚Ä¢ Enhanced ensemble: Random Forest + Gradient Boosting + SVM")
    print("   ‚Ä¢ Intelligent feature selection (mutual information)")
    print("   ‚Ä¢ 10-fold cross-validation")
    print("   ‚Ä¢ Robust scaling for outliers")
    print("   ‚Ä¢ Class balancing for imbalanced data")
    print()
    print("üìä ANALYSIS & VISUALIZATION:")
    print("   ‚Ä¢ MFCC heatmaps and line plots")
    print("   ‚Ä¢ Spectral feature time series")
    print("   ‚Ä¢ Chroma analysis by musical notes")
    print("   ‚Ä¢ Zero crossing rate histograms")
    print("   ‚Ä¢ Tempo estimation with beat detection")
    print("   ‚Ä¢ Feature comparison plots")
    print()
    print("üí° QUALITY FEATURES:")
    print("   ‚Ä¢ Confidence assessment")
    print("   ‚Ä¢ Individual model predictions")
    print("   ‚Ä¢ Feature quality scoring")
    print("   ‚Ä¢ Missing feature handling")
    print("   ‚Ä¢ Comprehensive JSON output")
    print("="*60)

# Fixed wrapper functions with correct names
def quick_predict(file_path, classifier=None):
    """Quick prediction wrapper (compatibility)"""
    return enhanced_quick_predict(file_path, classifier)

def show_model_info(performance=None):
    """Show model info wrapper (compatibility)"""
    return show_enhanced_model_info(performance)

def interactive_mode(classifier):
    """Interactive mode wrapper (compatibility)"""
    return enhanced_interactive_mode(classifier)

print("‚úÖ Part 8: Complete execution script (Fixed and Enhanced) ready!")
print("\nüéØ QUICK START:")
print("Just run this cell to start the complete enhanced pipeline!")
print("\nüîß ENHANCED FUNCTIONS:")
print("‚Ä¢ test_system() - Test all enhanced components")
print("‚Ä¢ show_enhanced_model_info(performance) - Show detailed model info")
print("‚Ä¢ enhanced_quick_predict(file_path, classifier) - Single enhanced prediction")
print("‚Ä¢ enhanced_interactive_mode(classifier) - Interactive mode with quality assessment")
print("‚Ä¢ show_feature_libraries() - Show feature extraction libraries")
print("‚Ä¢ show_system_capabilities() - Show all system capabilities")
print("\nüîÑ COMPATIBILITY FUNCTIONS:")
print("‚Ä¢ quick_predict() - Wrapper for enhanced_quick_predict()")
print("‚Ä¢ show_model_info() - Wrapper for show_enhanced_model_info()")
print("‚Ä¢ interactive_mode() - Wrapper for enhanced_interactive_mode()")

# Instructions
print("\n" + "="*80)
print("üìñ COMPLETE ENHANCED USAGE GUIDE")
print("="*80)
print("1. üìÅ SETUP:")
print("   ‚Ä¢ Install: pip install praat-parselmouth speechpy scikit-learn pandas numpy matplotlib seaborn librosa scipy")
print("   ‚Ä¢ Place your datasets: data/Dataset.csv and data/Final_dataset_s.csv")
print("")
print("2. üöÄ RUN:")
print("   ‚Ä¢ Execute this cell to start the complete enhanced pipeline")
print("   ‚Ä¢ The system will train automatically with intelligent feature selection")
print("   ‚Ä¢ Enhanced accuracy through optimized ensemble methods")
print("")
print("3. üéµ PREDICT:")
print("   ‚Ä¢ Use enhanced_interactive_mode() for multiple predictions with quality assessment")
print("   ‚Ä¢ Use enhanced_quick_predict() for single file predictions")
print("   ‚Ä¢ Get confidence scores, individual model predictions, and quality metrics")
print("")
print("4. üìä ENHANCED RESULTS:")
print("   ‚Ä¢ Check results/graphs/ for enhanced visualizations")
print("   ‚Ä¢ Check results/json_values/ for detailed JSON data with quality metrics")
print("   ‚Ä¢ Feature visualizations created for each audio file processed")
print("   ‚Ä¢ Quality assessment and confidence scoring")
print("")
print("5. üé® ENHANCED VISUALIZATIONS INCLUDE:")
print("   ‚Ä¢ MFCCs ‚Üí Heatmap + Line Plot + Statistical Analysis")
print("   ‚Ä¢ Spectral Centroid ‚Üí Time Series with Mean Line")
print("   ‚Ä¢ Spectral Bandwidth ‚Üí Time Series with Statistics") 
print("   ‚Ä¢ Chroma ‚Üí Heatmap + Musical Note Violin Plots")
print("   ‚Ä¢ Tempo ‚Üí Classification Bar Chart + Energy Curve + Beat Detection")
print("   ‚Ä¢ Zero Crossing Rate ‚Üí Time Series + Histogram + Statistical Analysis")
print("   ‚Ä¢ Feature Comparison ‚Üí Type-based Comprehensive Analysis")
print("")
print("6. üß† INTELLIGENCE FEATURES:")
print("   ‚Ä¢ Automatic feature selection using mutual information")
print("   ‚Ä¢ Smart feature alignment between training and prediction")
print("   ‚Ä¢ Quality scoring for predictions")
print("   ‚Ä¢ Robust handling of missing features")
print("   ‚Ä¢ Enhanced ensemble with weighted voting")
print("="*80)
